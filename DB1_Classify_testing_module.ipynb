{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# 資料預處理用\n",
    "import math\n",
    "import pywt\n",
    "# 讓每一種標籤的資料都平均一點\n",
    "import random\n",
    "# 機器學習相關\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "# 正規化\n",
    "from sklearn import preprocessing\n",
    "# 訓練結果報告分析\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料的模組\n",
    "def data_reading(path):\n",
    "    print('【讀取檔案】')\n",
    "    data = []\n",
    "    for file in os.listdir(path):\n",
    "        data.append(sio.loadmat(path + '/' + file))\n",
    "        plt.close()\n",
    "    print('檔案讀取完畢！共讀取了', len(data), '個檔案的資料\\n')\n",
    "    return data\n",
    "# 讀取受試者資料\n",
    "def subject_choose(data, subject_ID_list):\n",
    "    print('【讀取受試者資料】')\n",
    "    # subject_emg[受試者編號][emg編號][訊號]\n",
    "    # subject_emg[受試者編號][狀態]\n",
    "    subject_emg = []\n",
    "    subject_restimulus = []\n",
    "    # 選擇受試者\n",
    "    for subject_ID in subject_ID_list:\n",
    "        print('受試者', subject_ID, '資料讀取')\n",
    "        sub_data = list(filter(lambda x: np.squeeze(x['subject']) == subject_ID, data))[0]\n",
    "        subject_emg.append(sub_data['emg'].T)\n",
    "        subject_restimulus.append(sub_data['restimulus'])\n",
    "    print('受試者資料讀取完畢！\\n')\n",
    "    return subject_emg, subject_restimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料預處理\n",
    "\n",
    "def wavelet_with_energy_sum(sliding_window_size, sliding_duration, wavelet, mode, maxlevel):\n",
    "    print('【預處理-小波包能量】')\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    count = 0\n",
    "    for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "        count += 1\n",
    "        print('第' + str(count) + '位受試者資料預處理')\n",
    "        total_len = len(restimulus)\n",
    "        # math.ceil -> 無條件進位\n",
    "        sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "\n",
    "        # 資料分割 + 特徵提取\n",
    "        window_begin = 0\n",
    "        for i in range(sliding_times):\n",
    "        #   特徵提取\n",
    "            feature_matrix = []\n",
    "            for e in emg:\n",
    "                emg_segment = e[window_begin:window_begin+sliding_window_size]\n",
    "            #   使用多階小波包轉換\n",
    "            #   小波包基底: db5\n",
    "            #   層數: 4\n",
    "                wp = pywt.WaveletPacket(data=emg_segment, wavelet=wavelet, mode=mode, maxlevel=maxlevel)\n",
    "            #   對第四層每一節點做能量值計算\n",
    "                wavelet_energy = []\n",
    "                for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                    wavelet_energy.append(np.sum( (np.array(wp[j].data)) ** 2 ))\n",
    "                feature_matrix.append(wavelet_energy)\n",
    "            x_data.append(feature_matrix)\n",
    "        #   標標籤\n",
    "            restimulus_segment = restimulus[window_begin:window_begin+sliding_window_size]\n",
    "        #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "            counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "            #返回眾數(注意:此方法只有非負數列才可使用)\n",
    "            label_action_ID = np.argmax(counts)\n",
    "            y_data.append(label_action_ID)\n",
    "            window_begin = window_begin + sliding_duration\n",
    "    print('資料預處理完畢！共', len(x_data), '筆資料\\n')\n",
    "    print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))\n",
    "#     讓 label 0 的資料減少一點，使資料分布平均\n",
    "    x_filter_data = []\n",
    "    y_filter_data = []\n",
    "    for i in range(len(x_data)):\n",
    "        if y_data[i] == 0:\n",
    "            if random.randint(1, round(np.bincount(np.squeeze(y_data))[0]/np.bincount(np.squeeze(y_data))[1]) ) == 1:\n",
    "                x_filter_data.append(x_data[i])\n",
    "                y_filter_data.append(y_data[i])\n",
    "        else:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    x_data = x_filter_data\n",
    "    y_data = y_filter_data\n",
    "    del x_filter_data\n",
    "    del y_filter_data\n",
    "    print('\\n資料篩選完成')\n",
    "    print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "    print('資料標籤分佈：', np.bincount(np.squeeze(y_data)),'\\n')\n",
    "    # 正規化\n",
    "    x_data = list(preprocessing.scale(np.array(x_data).reshape(-1)).reshape(-1, 10, 16))\n",
    "    for i in range(len(x_data)):\n",
    "        x_data[i] = [x_data[i]]\n",
    "    return x_data, y_data\n",
    "\n",
    "def wavelet_parameters_only(sliding_window_size, sliding_duration, wavelet, mode, maxlevel):\n",
    "    print('【預處理-小波包係數】')\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    count = 0\n",
    "    for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "        count += 1\n",
    "        print('第' + str(count) + '位受試者資料預處理')\n",
    "        total_len = len(restimulus)\n",
    "        # math.ceil -> 無條件進位\n",
    "        sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "\n",
    "        # 資料分割 + 特徵提取\n",
    "        window_begin = 0\n",
    "        for i in range(sliding_times):\n",
    "        #   特徵提取\n",
    "            feature_matrix = []\n",
    "            for e in emg:\n",
    "                emg_segment = e[window_begin:window_begin+sliding_window_size]\n",
    "            #   使用多階小波包轉換\n",
    "            #   小波包基底: db5\n",
    "            #   層數: 4\n",
    "                wp = pywt.WaveletPacket(data=emg_segment, wavelet=wavelet, mode=mode, maxlevel=maxlevel)\n",
    "                wavelet_parameter = []\n",
    "                for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                    wavelet_parameter.append(wp[j].data)\n",
    "                feature_matrix.append(wavelet_parameter)\n",
    "            x_data.append(feature_matrix)\n",
    "        #   標標籤\n",
    "            restimulus_segment = restimulus[window_begin:window_begin+sliding_window_size]\n",
    "        #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "            counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "            #返回眾數(注意:此方法只有非負數列才可使用)\n",
    "            label_action_ID = np.argmax(counts)\n",
    "            y_data.append(label_action_ID)\n",
    "            window_begin = window_begin + sliding_duration\n",
    "    print('資料預處理完畢！共', len(x_data), '筆資料\\n')\n",
    "    print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))\n",
    "#     讓 label 0 的資料減少一點，使資料分布平均\n",
    "    x_filter_data = []\n",
    "    y_filter_data = []\n",
    "    for i in range(len(x_data)):\n",
    "        if y_data[i] == 0:\n",
    "            if random.randint(1, round(np.bincount(np.squeeze(y_data))[0]/np.bincount(np.squeeze(y_data))[1]) ) == 1:\n",
    "                x_filter_data.append(x_data[i])\n",
    "                y_filter_data.append(y_data[i])\n",
    "        else:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    x_data = x_filter_data\n",
    "    y_data = y_filter_data\n",
    "    del x_filter_data\n",
    "    del y_filter_data\n",
    "    print('\\n資料篩選完成')\n",
    "    print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "    print('資料標籤分佈：', np.bincount(np.squeeze(y_data)),'\\n')\n",
    "    # 正規化\n",
    "    print(np.array(x_data).shape)\n",
    "    x_data = list(preprocessing.scale(np.array(x_data).reshape(-1)).reshape(-1, 10, 16, 20))\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機器學習模組\n",
    "\n",
    "# 資料集設定\n",
    "def data_setting(BATCH_SIZE):\n",
    "    print('【資料集設定】')\n",
    "    # 先转换成 torch 能识别的 Dataset\n",
    "    torch_dataset = Data.TensorDataset(torch.FloatTensor(x_data), torch.LongTensor(y_data))\n",
    "\n",
    "    # 把 dataset 放入 DataLoader\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    )\n",
    "    return loader\n",
    "# CNN_energy\n",
    "class CNN_energy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_energy, self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential( # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1, # 輸入信號的通道\n",
    "                out_channels = 4, # 卷積產生的通道\n",
    "                kernel_size = (3, 5), # 卷積核的尺寸\n",
    "                stride = 1, # 卷積步長\n",
    "                padding = (1,2) # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(4, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(4, 8, (3,5), 1, (1,2)), # output shape(8, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(8*10*16, 800)\n",
    "        self.hidden2 = nn.Linear(800, 800)\n",
    "        self.hidden3 = nn.Linear(800, 800)\n",
    "        self.out = nn.Linear(800, 13) # fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "# CNN_parameter\n",
    "class CNN_parameter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_parameter, self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential( # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels = 10, # 輸入信號的通道\n",
    "                out_channels = 20, # 卷積產生的通道\n",
    "                kernel_size = (3, 5), # 卷積核的尺寸\n",
    "                stride = 1, # 卷積步長\n",
    "                padding = (1,2) # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(20, 16, 20)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(20, 40, (3,5), 1, (1,2)), # output shape(40, 16, 20)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(40*16*20, 800)\n",
    "        self.hidden2 = nn.Linear(800, 800)\n",
    "        self.hidden3 = nn.Linear(800, 800)\n",
    "        self.out = nn.Linear(800, 13) # fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "# 訓練模型\n",
    "def module_training(EPOCH, LR, module_class):\n",
    "    print('【訓練模型】')\n",
    "    module = module_class()\n",
    "    print(module)\n",
    "    # optimize all cnn parameters\n",
    "    optimizer = torch.optim.Adam(module.parameters(), lr=LR)\n",
    "    # the target label is not one-hotted\n",
    "    # pytorch 的 CrossEntropyLoss 會自動把張量轉為 one hot形式\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training and testing\n",
    "    for epoch in range(EPOCH):\n",
    "        print('第', epoch, '次訓練')\n",
    "        # enumerate : 枚舉可列舉對象．ex.\n",
    "        # A = [a, b, c]\n",
    "        # list(enumerate(A)) = [(0,a), (1,b), (2,c)]\n",
    "        for step, (b_x, b_y) in enumerate(loader):\n",
    "            output = module(b_x)\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step % 50 == 0):\n",
    "                print('step:' + str(step))\n",
    "                print('loss:' + str(loss))\n",
    "    print('訓練完成！！\\n')\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練結果分析\n",
    "def testing_report(module, testing_x_data, testing_y_data):\n",
    "    print('【訓練結果報告】')\n",
    "    test_output = module(torch.FloatTensor(testing_x_data))\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    # 混淆矩陣\n",
    "    confusion_matrix = metrics.confusion_matrix(testing_y_data,pred_y)\n",
    "    print('混淆矩陣')\n",
    "    print('true/predict')\n",
    "    print(confusion_matrix)\n",
    "    classification_report = metrics.classification_report(testing_y_data,pred_y)\n",
    "    print('\\n<============================>\\n\\n分類報告')\n",
    "    print(classification_report)\n",
    "\n",
    "    print('<============================>\\n\\n分類準確率')\n",
    "    correct = 0\n",
    "    for i in range(len(testing_y_data)):\n",
    "        if testing_y_data[i] == pred_y[i]:\n",
    "            correct += 1\n",
    "    print('測試資料數：', len(testing_y_data), ', 預測正確數：', correct, '準確率：', (correct/len(testing_y_data)))\n",
    "    return confusion_matrix, classification_report, (correct/len(testing_y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【讀取檔案】\n",
      "檔案讀取完畢！共讀取了 27 個檔案的資料\n",
      "\n",
      "【讀取受試者資料】\n",
      "受試者 4 資料讀取\n",
      "受試者 7 資料讀取\n",
      "受試者資料讀取完畢！\n",
      "\n",
      "【預處理-小波包能量】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "資料預處理完畢！共 2016 筆資料\n",
      "\n",
      "資料標籤數量分布： [1127   78   81   71   71   68   80   79   75   75   81   59   71]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  965 , y -> 965\n",
      "資料標籤分佈： [76 78 81 71 71 68 80 79 75 75 81 59 71] \n",
      "\n",
      "【資料集設定】\n",
      "【訓練模型】\n",
      "CNN_energy(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(4, 8, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=1280, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5649, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5591, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5482, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5360, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5252, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5105, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4986, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4823, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4666, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4569, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4475, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4364, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4240, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4117, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4000, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3870, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3836, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3760, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3661, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3556, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3401, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3318, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3295, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3128, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3083, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3015, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2919, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2899, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2846, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2778, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2754, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2739, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2663, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2639, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2612, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2586, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2540, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2505, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2446, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2426, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2417, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2396, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2346, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2321, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2304, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2289, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2264, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2235, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2219, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2200, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2155, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2160, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2166, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2119, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2094, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2059, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2042, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2031, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2003, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1987, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1998, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1930, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2019, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1958, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1986, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1893, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1891, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1903, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1861, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1859, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1810, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1843, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1782, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1803, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1748, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1779, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1761, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1763, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1679, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1668, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1646, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1688, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1581, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1612, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1569, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1573, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1532, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1523, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1498, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1491, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1482, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1464, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1468, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1442, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1438, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1422, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1399, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1385, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1377, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1374, grad_fn=<NllLossBackward>)\n",
      "第 100 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1358, grad_fn=<NllLossBackward>)\n",
      "第 101 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1377, grad_fn=<NllLossBackward>)\n",
      "第 102 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1397, grad_fn=<NllLossBackward>)\n",
      "第 103 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1510, grad_fn=<NllLossBackward>)\n",
      "第 104 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1434, grad_fn=<NllLossBackward>)\n",
      "第 105 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1497, grad_fn=<NllLossBackward>)\n",
      "第 106 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1509, grad_fn=<NllLossBackward>)\n",
      "第 107 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1370, grad_fn=<NllLossBackward>)\n",
      "第 108 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1489, grad_fn=<NllLossBackward>)\n",
      "第 109 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1330, grad_fn=<NllLossBackward>)\n",
      "第 110 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1366, grad_fn=<NllLossBackward>)\n",
      "第 111 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1311, grad_fn=<NllLossBackward>)\n",
      "第 112 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1313, grad_fn=<NllLossBackward>)\n",
      "第 113 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1388, grad_fn=<NllLossBackward>)\n",
      "第 114 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1372, grad_fn=<NllLossBackward>)\n",
      "第 115 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1266, grad_fn=<NllLossBackward>)\n",
      "第 116 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1324, grad_fn=<NllLossBackward>)\n",
      "第 117 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1228, grad_fn=<NllLossBackward>)\n",
      "第 118 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1259, grad_fn=<NllLossBackward>)\n",
      "第 119 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1189, grad_fn=<NllLossBackward>)\n",
      "第 120 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1181, grad_fn=<NllLossBackward>)\n",
      "第 121 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1116, grad_fn=<NllLossBackward>)\n",
      "第 122 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1145, grad_fn=<NllLossBackward>)\n",
      "第 123 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1104, grad_fn=<NllLossBackward>)\n",
      "第 124 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1074, grad_fn=<NllLossBackward>)\n",
      "第 125 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1032, grad_fn=<NllLossBackward>)\n",
      "第 126 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1100, grad_fn=<NllLossBackward>)\n",
      "第 127 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1063, grad_fn=<NllLossBackward>)\n",
      "第 128 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1112, grad_fn=<NllLossBackward>)\n",
      "第 129 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1072, grad_fn=<NllLossBackward>)\n",
      "第 130 次訓練\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.0974, grad_fn=<NllLossBackward>)\n",
      "第 131 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1076, grad_fn=<NllLossBackward>)\n",
      "第 132 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1094, grad_fn=<NllLossBackward>)\n",
      "第 133 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1154, grad_fn=<NllLossBackward>)\n",
      "第 134 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1178, grad_fn=<NllLossBackward>)\n",
      "第 135 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1121, grad_fn=<NllLossBackward>)\n",
      "第 136 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0997, grad_fn=<NllLossBackward>)\n",
      "第 137 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1064, grad_fn=<NllLossBackward>)\n",
      "第 138 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1222, grad_fn=<NllLossBackward>)\n",
      "第 139 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0942, grad_fn=<NllLossBackward>)\n",
      "第 140 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1067, grad_fn=<NllLossBackward>)\n",
      "第 141 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1029, grad_fn=<NllLossBackward>)\n",
      "第 142 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1035, grad_fn=<NllLossBackward>)\n",
      "第 143 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1059, grad_fn=<NllLossBackward>)\n",
      "第 144 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1077, grad_fn=<NllLossBackward>)\n",
      "第 145 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1092, grad_fn=<NllLossBackward>)\n",
      "第 146 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1082, grad_fn=<NllLossBackward>)\n",
      "第 147 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1042, grad_fn=<NllLossBackward>)\n",
      "第 148 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1077, grad_fn=<NllLossBackward>)\n",
      "第 149 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1040, grad_fn=<NllLossBackward>)\n",
      "第 150 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1061, grad_fn=<NllLossBackward>)\n",
      "第 151 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1014, grad_fn=<NllLossBackward>)\n",
      "第 152 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1035, grad_fn=<NllLossBackward>)\n",
      "第 153 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1075, grad_fn=<NllLossBackward>)\n",
      "第 154 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1019, grad_fn=<NllLossBackward>)\n",
      "第 155 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0981, grad_fn=<NllLossBackward>)\n",
      "第 156 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0984, grad_fn=<NllLossBackward>)\n",
      "第 157 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0972, grad_fn=<NllLossBackward>)\n",
      "第 158 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0975, grad_fn=<NllLossBackward>)\n",
      "第 159 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0963, grad_fn=<NllLossBackward>)\n",
      "第 160 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0977, grad_fn=<NllLossBackward>)\n",
      "第 161 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0976, grad_fn=<NllLossBackward>)\n",
      "第 162 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0961, grad_fn=<NllLossBackward>)\n",
      "第 163 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0947, grad_fn=<NllLossBackward>)\n",
      "第 164 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0951, grad_fn=<NllLossBackward>)\n",
      "第 165 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0940, grad_fn=<NllLossBackward>)\n",
      "第 166 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0946, grad_fn=<NllLossBackward>)\n",
      "第 167 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0936, grad_fn=<NllLossBackward>)\n",
      "第 168 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0922, grad_fn=<NllLossBackward>)\n",
      "第 169 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0919, grad_fn=<NllLossBackward>)\n",
      "第 170 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0913, grad_fn=<NllLossBackward>)\n",
      "第 171 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0915, grad_fn=<NllLossBackward>)\n",
      "第 172 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0911, grad_fn=<NllLossBackward>)\n",
      "第 173 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0905, grad_fn=<NllLossBackward>)\n",
      "第 174 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0898, grad_fn=<NllLossBackward>)\n",
      "第 175 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0891, grad_fn=<NllLossBackward>)\n",
      "第 176 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0891, grad_fn=<NllLossBackward>)\n",
      "第 177 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0882, grad_fn=<NllLossBackward>)\n",
      "第 178 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0883, grad_fn=<NllLossBackward>)\n",
      "第 179 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0876, grad_fn=<NllLossBackward>)\n",
      "第 180 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0871, grad_fn=<NllLossBackward>)\n",
      "第 181 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0874, grad_fn=<NllLossBackward>)\n",
      "第 182 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0865, grad_fn=<NllLossBackward>)\n",
      "第 183 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0862, grad_fn=<NllLossBackward>)\n",
      "第 184 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0859, grad_fn=<NllLossBackward>)\n",
      "第 185 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0856, grad_fn=<NllLossBackward>)\n",
      "第 186 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0854, grad_fn=<NllLossBackward>)\n",
      "第 187 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0849, grad_fn=<NllLossBackward>)\n",
      "第 188 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0848, grad_fn=<NllLossBackward>)\n",
      "第 189 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0844, grad_fn=<NllLossBackward>)\n",
      "第 190 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0842, grad_fn=<NllLossBackward>)\n",
      "第 191 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0839, grad_fn=<NllLossBackward>)\n",
      "第 192 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0835, grad_fn=<NllLossBackward>)\n",
      "第 193 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0833, grad_fn=<NllLossBackward>)\n",
      "第 194 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0830, grad_fn=<NllLossBackward>)\n",
      "第 195 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0827, grad_fn=<NllLossBackward>)\n",
      "第 196 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0825, grad_fn=<NllLossBackward>)\n",
      "第 197 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0822, grad_fn=<NllLossBackward>)\n",
      "第 198 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0820, grad_fn=<NllLossBackward>)\n",
      "第 199 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0818, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[34  4  1  6  0  1  5  0  0  5 10  0 10]\n",
      " [ 0 77  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  4 76  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 69  1  0  0  0  0  0  0  0  0]\n",
      " [ 2  3  0  4 32  0 30  0  0  0  0  0  0]\n",
      " [ 2  3  2 26  0 31  4  0  0  0  0  0  0]\n",
      " [ 2  0  0  2  0  0 73  1  0  1  0  0  1]\n",
      " [ 2 10  0  4  1  0 23 39  0  0  0  0  0]\n",
      " [17  0  1 26  2  5 13  0  0  3  2  0  6]\n",
      " [ 3  0 16  4  0  0  2  0  0 46  4  0  0]\n",
      " [ 1  0  2  0  0  0  0  0  0 19 53  0  6]\n",
      " [ 0  0 28  0  0  1  0  0  0  9 21  0  0]\n",
      " [ 0  0  0  1  0  1  0  0  0  5  1  0 63]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.45      0.49        76\n",
      "           1       0.75      0.99      0.86        78\n",
      "           2       0.60      0.94      0.73        81\n",
      "           3       0.48      0.97      0.64        71\n",
      "           4       0.89      0.45      0.60        71\n",
      "           5       0.79      0.46      0.58        68\n",
      "           6       0.49      0.91      0.63        80\n",
      "           7       0.97      0.49      0.66        79\n",
      "           8       0.00      0.00      0.00        75\n",
      "           9       0.52      0.61      0.56        75\n",
      "          10       0.58      0.65      0.62        81\n",
      "          11       0.00      0.00      0.00        59\n",
      "          12       0.73      0.89      0.80        71\n",
      "\n",
      "    accuracy                           0.61       965\n",
      "   macro avg       0.57      0.60      0.55       965\n",
      "weighted avg       0.57      0.61      0.56       965\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 965 , 預測正確數： 593 準確率： 0.6145077720207254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6145077720207254"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_reading('S_All_A1_E1')\n",
    "subject_emg, subject_restimulus = subject_choose(data, [4,7])\n",
    "x_data, y_data = wavelet_with_energy_sum(200, 100, 'db5', 'symmetric', 4)\n",
    "loader = data_setting(1024)\n",
    "module = module_training(200, 0.0005, CNN_energy)\n",
    "\n",
    "begin = 0\n",
    "end = len(y_data)\n",
    "testing_report(module, x_data[begin:end], y_data[begin:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【讀取檔案】\n",
      "檔案讀取完畢！共讀取了 27 個檔案的資料\n",
      "\n",
      "【讀取受試者資料】\n",
      "受試者 1 資料讀取\n",
      "受試者 2 資料讀取\n",
      "受試者 3 資料讀取\n",
      "受試者 4 資料讀取\n",
      "受試者 5 資料讀取\n",
      "受試者 6 資料讀取\n",
      "受試者 7 資料讀取\n",
      "受試者 8 資料讀取\n",
      "受試者 9 資料讀取\n",
      "受試者 10 資料讀取\n",
      "受試者 11 資料讀取\n",
      "受試者 12 資料讀取\n",
      "受試者 13 資料讀取\n",
      "受試者 14 資料讀取\n",
      "受試者 15 資料讀取\n",
      "受試者 16 資料讀取\n",
      "受試者 17 資料讀取\n",
      "受試者 18 資料讀取\n",
      "受試者 19 資料讀取\n",
      "受試者 20 資料讀取\n",
      "受試者 21 資料讀取\n",
      "受試者 22 資料讀取\n",
      "受試者 23 資料讀取\n",
      "受試者 24 資料讀取\n",
      "受試者 25 資料讀取\n",
      "受試者資料讀取完畢！\n",
      "\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "資料預處理完畢！共 25274 筆資料\n",
      "\n",
      "資料標籤數量分布： [14379   951   942  1017   873   888   907   941   962   852   834   814\n",
      "   914]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  11899 , y -> 11899\n",
      "資料標籤分佈： [1004  951  942 1017  873  888  907  941  962  852  834  814  914] \n",
      "\n",
      "(11899, 10, 16, 20)\n",
      "【資料集設定】\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=12800, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5651, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4492, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4175, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3640, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3557, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3608, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3248, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2941, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3227, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2824, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2677, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2537, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2616, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2503, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2529, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2272, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2144, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2297, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2091, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2082, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1671, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1613, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1410, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1890, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1799, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1454, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1604, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1716, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1411, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1320, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1467, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1488, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1302, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1335, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1145, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0783, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0989, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0742, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1028, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0817, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0542, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0639, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0406, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0466, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0469, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0483, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0548, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0449, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0308, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0389, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0273, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0053, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0396, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0345, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0279, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0377, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0173, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0220, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0348, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0122, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0139, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9931, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9974, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9878, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9978, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9872, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9708, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0001, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9937, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9601, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9899, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9695, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9895, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9738, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9728, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9692, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9547, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9901, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9573, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9473, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9614, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9663, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9595, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9577, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9473, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9485, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9403, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9359, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9577, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9436, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9325, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9378, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9521, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9528, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9463, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9549, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9299, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9135, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9324, grad_fn=<NllLossBackward>)\n",
      "第 100 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9237, grad_fn=<NllLossBackward>)\n",
      "第 101 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9257, grad_fn=<NllLossBackward>)\n",
      "第 102 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9532, grad_fn=<NllLossBackward>)\n",
      "第 103 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9240, grad_fn=<NllLossBackward>)\n",
      "第 104 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9010, grad_fn=<NllLossBackward>)\n",
      "第 105 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9383, grad_fn=<NllLossBackward>)\n",
      "第 106 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9469, grad_fn=<NllLossBackward>)\n",
      "第 107 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9107, grad_fn=<NllLossBackward>)\n",
      "第 108 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9372, grad_fn=<NllLossBackward>)\n",
      "第 109 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9328, grad_fn=<NllLossBackward>)\n",
      "第 110 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9320, grad_fn=<NllLossBackward>)\n",
      "第 111 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9475, grad_fn=<NllLossBackward>)\n",
      "第 112 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9233, grad_fn=<NllLossBackward>)\n",
      "第 113 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9085, grad_fn=<NllLossBackward>)\n",
      "第 114 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9400, grad_fn=<NllLossBackward>)\n",
      "第 115 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8902, grad_fn=<NllLossBackward>)\n",
      "第 116 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8940, grad_fn=<NllLossBackward>)\n",
      "第 117 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9215, grad_fn=<NllLossBackward>)\n",
      "第 118 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9079, grad_fn=<NllLossBackward>)\n",
      "第 119 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8925, grad_fn=<NllLossBackward>)\n",
      "第 120 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8887, grad_fn=<NllLossBackward>)\n",
      "第 121 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9207, grad_fn=<NllLossBackward>)\n",
      "第 122 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9079, grad_fn=<NllLossBackward>)\n",
      "第 123 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9179, grad_fn=<NllLossBackward>)\n",
      "第 124 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8932, grad_fn=<NllLossBackward>)\n",
      "第 125 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9073, grad_fn=<NllLossBackward>)\n",
      "第 126 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9202, grad_fn=<NllLossBackward>)\n",
      "第 127 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9163, grad_fn=<NllLossBackward>)\n",
      "第 128 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9085, grad_fn=<NllLossBackward>)\n",
      "第 129 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9032, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 130 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9097, grad_fn=<NllLossBackward>)\n",
      "第 131 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9036, grad_fn=<NllLossBackward>)\n",
      "第 132 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9060, grad_fn=<NllLossBackward>)\n",
      "第 133 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8990, grad_fn=<NllLossBackward>)\n",
      "第 134 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8977, grad_fn=<NllLossBackward>)\n",
      "第 135 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9088, grad_fn=<NllLossBackward>)\n",
      "第 136 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9051, grad_fn=<NllLossBackward>)\n",
      "第 137 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9197, grad_fn=<NllLossBackward>)\n",
      "第 138 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9268, grad_fn=<NllLossBackward>)\n",
      "第 139 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8932, grad_fn=<NllLossBackward>)\n",
      "第 140 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9211, grad_fn=<NllLossBackward>)\n",
      "第 141 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8981, grad_fn=<NllLossBackward>)\n",
      "第 142 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9099, grad_fn=<NllLossBackward>)\n",
      "第 143 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8984, grad_fn=<NllLossBackward>)\n",
      "第 144 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9088, grad_fn=<NllLossBackward>)\n",
      "第 145 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8930, grad_fn=<NllLossBackward>)\n",
      "第 146 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9132, grad_fn=<NllLossBackward>)\n",
      "第 147 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8995, grad_fn=<NllLossBackward>)\n",
      "第 148 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8943, grad_fn=<NllLossBackward>)\n",
      "第 149 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8858, grad_fn=<NllLossBackward>)\n",
      "第 150 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8897, grad_fn=<NllLossBackward>)\n",
      "第 151 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8881, grad_fn=<NllLossBackward>)\n",
      "第 152 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9003, grad_fn=<NllLossBackward>)\n",
      "第 153 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8955, grad_fn=<NllLossBackward>)\n",
      "第 154 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8845, grad_fn=<NllLossBackward>)\n",
      "第 155 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9007, grad_fn=<NllLossBackward>)\n",
      "第 156 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "第 157 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9185, grad_fn=<NllLossBackward>)\n",
      "第 158 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8866, grad_fn=<NllLossBackward>)\n",
      "第 159 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8900, grad_fn=<NllLossBackward>)\n",
      "第 160 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9280, grad_fn=<NllLossBackward>)\n",
      "第 161 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8891, grad_fn=<NllLossBackward>)\n",
      "第 162 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9054, grad_fn=<NllLossBackward>)\n",
      "第 163 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8984, grad_fn=<NllLossBackward>)\n",
      "第 164 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8850, grad_fn=<NllLossBackward>)\n",
      "第 165 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9006, grad_fn=<NllLossBackward>)\n",
      "第 166 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8896, grad_fn=<NllLossBackward>)\n",
      "第 167 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8917, grad_fn=<NllLossBackward>)\n",
      "第 168 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9032, grad_fn=<NllLossBackward>)\n",
      "第 169 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8579, grad_fn=<NllLossBackward>)\n",
      "第 170 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8969, grad_fn=<NllLossBackward>)\n",
      "第 171 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8873, grad_fn=<NllLossBackward>)\n",
      "第 172 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8791, grad_fn=<NllLossBackward>)\n",
      "第 173 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8785, grad_fn=<NllLossBackward>)\n",
      "第 174 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8720, grad_fn=<NllLossBackward>)\n",
      "第 175 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9005, grad_fn=<NllLossBackward>)\n",
      "第 176 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9162, grad_fn=<NllLossBackward>)\n",
      "第 177 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8862, grad_fn=<NllLossBackward>)\n",
      "第 178 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9224, grad_fn=<NllLossBackward>)\n",
      "第 179 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9296, grad_fn=<NllLossBackward>)\n",
      "第 180 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9230, grad_fn=<NllLossBackward>)\n",
      "第 181 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8942, grad_fn=<NllLossBackward>)\n",
      "第 182 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8792, grad_fn=<NllLossBackward>)\n",
      "第 183 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8761, grad_fn=<NllLossBackward>)\n",
      "第 184 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8955, grad_fn=<NllLossBackward>)\n",
      "第 185 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8867, grad_fn=<NllLossBackward>)\n",
      "第 186 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8980, grad_fn=<NllLossBackward>)\n",
      "第 187 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8934, grad_fn=<NllLossBackward>)\n",
      "第 188 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9032, grad_fn=<NllLossBackward>)\n",
      "第 189 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8917, grad_fn=<NllLossBackward>)\n",
      "第 190 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8781, grad_fn=<NllLossBackward>)\n",
      "第 191 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8740, grad_fn=<NllLossBackward>)\n",
      "第 192 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8816, grad_fn=<NllLossBackward>)\n",
      "第 193 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9003, grad_fn=<NllLossBackward>)\n",
      "第 194 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8718, grad_fn=<NllLossBackward>)\n",
      "第 195 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8853, grad_fn=<NllLossBackward>)\n",
      "第 196 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8934, grad_fn=<NllLossBackward>)\n",
      "第 197 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8837, grad_fn=<NllLossBackward>)\n",
      "第 198 次訓練\n",
      "step:0\n",
      "loss:tensor(1.8932, grad_fn=<NllLossBackward>)\n",
      "第 199 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9025, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[816  22  19  30   4  14  11  13  12   9  30  24   0]\n",
      " [ 20 819  13  33   3   5   1  14  29   2   7   5   0]\n",
      " [  9   4 897  14   2   0   2   7   2   0   4   1   0]\n",
      " [  5  11   2 893  11   6  40  25  11   4   3   6   0]\n",
      " [  4  23   9  21 761   9   5  10   1   4   2  24   0]\n",
      " [ 24  17   7  50   0 753   5  12   9   4   3   4   0]\n",
      " [ 10  10  15  22  20   4 740  36  18  17   6   9   0]\n",
      " [  8   6   0  14   0  27   5 838  14  10   2  17   0]\n",
      " [ 22  12   7  35   4   0  23  17 822   4   6  10   0]\n",
      " [ 17  10   7  13  12  11   6   7  21 653  68  27   0]\n",
      " [ 27  23   0   3   0   0   0   1   4  11 750  15   0]\n",
      " [ 12   1   2   4   1   1   0   2   5   9   7 770   0]\n",
      " [ 98  55  91 111   8  50  14  38  45  94 171 139   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79      1004\n",
      "           1       0.81      0.86      0.83       951\n",
      "           2       0.84      0.95      0.89       942\n",
      "           3       0.72      0.88      0.79      1017\n",
      "           4       0.92      0.87      0.90       873\n",
      "           5       0.86      0.85      0.85       888\n",
      "           6       0.87      0.82      0.84       907\n",
      "           7       0.82      0.89      0.85       941\n",
      "           8       0.83      0.85      0.84       962\n",
      "           9       0.80      0.77      0.78       852\n",
      "          10       0.71      0.90      0.79       834\n",
      "          11       0.73      0.95      0.83       814\n",
      "          12       0.00      0.00      0.00       914\n",
      "\n",
      "    accuracy                           0.80     11899\n",
      "   macro avg       0.74      0.80      0.77     11899\n",
      "weighted avg       0.74      0.80      0.77     11899\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 11899 , 預測正確數： 9512 準確率： 0.7993949071350533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7993949071350533"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_reading('S_All_A1_E1')\n",
    "# [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "subject = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "subject_emg, subject_restimulus = subject_choose(data, subject)\n",
    "x_data, y_data = wavelet_parameters_only(200, 100, 'db5', 'symmetric', 4)\n",
    "loader = data_setting(1024)\n",
    "module = module_training(200, 0.0005, CNN_parameter)\n",
    "\n",
    "begin = 0 \n",
    "end = len(y_data)\n",
    "testing_report(module, x_data[begin:end], y_data[begin:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_training(epoch, batch_size, CNN_neuron_num, sliding_window_size, sliding_window_movement, feature_extract, learning_rate, subject):\n",
    "    data = data_reading('S_All_A1_E1')\n",
    "    subject_emg, subject_restimulus = subject_choose(data, subject)\n",
    "    x_data, y_data = wavelet_parameters_only(sliding_window_size, sliding_window_movement, 'db5', 'symmetric', 4)\n",
    "    loader = data_setting(batch_size)\n",
    "    module = module_training(epoch, learning_rate, CNN_parameter)\n",
    "\n",
    "    begin = 0 \n",
    "    end = len(y_data)\n",
    "    testing_report(module, x_data[begin:end], y_data[begin:end])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
