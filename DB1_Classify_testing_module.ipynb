{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# 資料預處理用\n",
    "import math\n",
    "import pywt\n",
    "# 讓每一種標籤的資料都平均一點\n",
    "import random\n",
    "# 資料分割用\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 機器學習相關\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "# 正規化\n",
    "from sklearn import preprocessing\n",
    "# 訓練結果報告分析\n",
    "from sklearn import metrics\n",
    "# 儲存報告(使用當前時間作為 pickle 檔名)\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料的模組\n",
    "def data_reading(path):\n",
    "    print('【讀取檔案】')\n",
    "    data = []\n",
    "    for file in os.listdir(path):\n",
    "        data.append(sio.loadmat(path + '/' + file))\n",
    "        plt.close()\n",
    "    print('檔案讀取完畢！共讀取了', len(data), '個檔案的資料\\n')\n",
    "    return data\n",
    "# 讀取受試者資料\n",
    "def subject_choose(data, subject_ID_list):\n",
    "    print('【讀取受試者資料】')\n",
    "    # subject_emg[受試者編號][emg編號][訊號]\n",
    "    # subject_emg[受試者編號][狀態]\n",
    "    subject_emg = []\n",
    "    subject_restimulus = []\n",
    "    # 選擇受試者\n",
    "    for subject_ID in subject_ID_list:\n",
    "        print('受試者', subject_ID, '資料讀取')\n",
    "        sub_data = list(filter(lambda x: np.squeeze(x['subject']) == subject_ID, data))[0]\n",
    "        subject_emg.append(sub_data['emg'].T)\n",
    "        subject_restimulus.append(sub_data['restimulus'])\n",
    "    print('受試者資料讀取完畢！\\n')\n",
    "    return subject_emg, subject_restimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料預處理\n",
    "\n",
    "def wavelet_with_energy_sum(subject_emg, subject_restimulus, sliding_window_size, sliding_duration, wavelet, mode, maxlevel):\n",
    "    print('【預處理-小波包能量】')\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    count = 0\n",
    "    for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "        count += 1\n",
    "        print('第' + str(count) + '位受試者資料預處理')\n",
    "        total_len = len(restimulus)\n",
    "        # math.ceil -> 無條件進位\n",
    "        sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "\n",
    "        # 資料分割 + 特徵提取\n",
    "        window_begin = 0\n",
    "        for i in range(sliding_times):\n",
    "        #   特徵提取\n",
    "            feature_matrix = []\n",
    "            for e in emg:\n",
    "                emg_segment = e[window_begin:window_begin+sliding_window_size]\n",
    "            #   使用多階小波包轉換\n",
    "            #   小波包基底: db5\n",
    "            #   層數: 4\n",
    "                wp = pywt.WaveletPacket(data=emg_segment, wavelet=wavelet, mode=mode, maxlevel=maxlevel)\n",
    "            #   對第四層每一節點做能量值計算\n",
    "                wavelet_energy = []\n",
    "                for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                    wavelet_energy.append(np.sum( (np.array(wp[j].data)) ** 2 ))\n",
    "                feature_matrix.append(wavelet_energy)\n",
    "            x_data.append(feature_matrix)\n",
    "        #   標標籤\n",
    "            restimulus_segment = restimulus[window_begin:window_begin+sliding_window_size]\n",
    "        #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "            counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "            #返回眾數(注意:此方法只有非負數列才可使用)\n",
    "            label_action_ID = np.argmax(counts)\n",
    "            y_data.append(label_action_ID)\n",
    "            window_begin = window_begin + sliding_duration\n",
    "    print('資料預處理完畢！共', len(x_data), '筆資料\\n')\n",
    "    print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))\n",
    "#     讓 label 0 的資料減少一點，使資料分布平均\n",
    "    x_filter_data = []\n",
    "    y_filter_data = []\n",
    "    for i in range(len(x_data)):\n",
    "        if y_data[i] == 0:\n",
    "            if random.randint(1, round(np.bincount(np.squeeze(y_data))[0]/np.bincount(np.squeeze(y_data))[1]) ) == 1:\n",
    "                x_filter_data.append(x_data[i])\n",
    "                y_filter_data.append(y_data[i])\n",
    "        else:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    x_data = x_filter_data\n",
    "    y_data = y_filter_data\n",
    "    del x_filter_data\n",
    "    del y_filter_data\n",
    "    print('\\n資料篩選完成')\n",
    "    print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "    print('資料標籤分佈：', np.bincount(np.squeeze(y_data)),'\\n')\n",
    "    # 正規化\n",
    "    x_data = list(preprocessing.scale(np.array(x_data).reshape(-1)).reshape(-1, 10, 16))\n",
    "    for i in range(len(x_data)):\n",
    "        x_data[i] = [x_data[i]]\n",
    "    return x_data, y_data\n",
    "\n",
    "def wavelet_parameters_only(subject_emg, subject_restimulus, sliding_window_size, sliding_duration, wavelet, mode,\n",
    "                            maxlevel):\n",
    "    print('【預處理-小波包係數】')\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    count = 0\n",
    "    # 確保特徵矩陣的 shape 都一樣\n",
    "    standard_feature_matrix_len1 = 0\n",
    "    standard_feature_matrix_len2 = 0\n",
    "    for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "        count += 1\n",
    "        print('第' + str(count) + '位受試者資料預處理')\n",
    "        total_len = len(restimulus)\n",
    "        # math.ceil -> 無條件進位\n",
    "        sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "        # 資料分割 + 特徵提取\n",
    "        window_begin = 0\n",
    "        for i in range(sliding_times):\n",
    "            #   特徵提取\n",
    "            feature_matrix = []\n",
    "            for e in emg:\n",
    "                emg_segment = e[window_begin:window_begin + sliding_window_size]\n",
    "                #   使用多階小波包轉換\n",
    "                #   小波包基底: db5\n",
    "                #   層數: 4\n",
    "                wp = pywt.WaveletPacket(data=emg_segment, wavelet=wavelet, mode=mode, maxlevel=maxlevel)\n",
    "                wavelet_parameter = []\n",
    "                for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                    wavelet_parameter.append(wp[j].data)\n",
    "                feature_matrix.append(wavelet_parameter)\n",
    "            #   標標籤\n",
    "            restimulus_segment = restimulus[window_begin:window_begin + sliding_window_size]\n",
    "            #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "            counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "            # 返回眾數(注意:此方法只有非負數列才可使用)\n",
    "            label_action_ID = np.argmax(counts)\n",
    "            # 特徵矩陣滿足標準尺寸（ shape ）才可進資料中\n",
    "            if i == 0:\n",
    "                standard_feature_matrix_len1 = len(feature_matrix[0])\n",
    "                standard_feature_matrix_len2 = len(feature_matrix[0][0])\n",
    "            if len(feature_matrix[0]) == standard_feature_matrix_len1 and len(feature_matrix[0][0]) == standard_feature_matrix_len2:\n",
    "                x_data.append(feature_matrix)\n",
    "                y_data.append(label_action_ID)\n",
    "            window_begin = window_begin + sliding_duration\n",
    "    print('資料預處理完畢！共', len(x_data), '筆資料\\n')\n",
    "    print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))\n",
    "    #     讓 label 0 的資料減少一點，使資料分布平均\n",
    "    x_filter_data = []\n",
    "    y_filter_data = []\n",
    "    for i in range(len(x_data)):\n",
    "        if y_data[i] == 0:\n",
    "            if random.randint(1, round(np.bincount(np.squeeze(y_data))[0] / np.bincount(np.squeeze(y_data))[1])) == 1:\n",
    "                x_filter_data.append(x_data[i])\n",
    "                y_filter_data.append(y_data[i])\n",
    "        else:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    x_data = x_filter_data\n",
    "    y_data = y_filter_data\n",
    "    del x_filter_data\n",
    "    del y_filter_data\n",
    "    print('\\n資料篩選完成')\n",
    "    print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "    print('資料標籤分佈：', np.bincount(np.squeeze(y_data)), '\\n')\n",
    "    # 正規化\n",
    "    print('正規化')\n",
    "    print(np.array(x_data).shape)\n",
    "    x_data = list(preprocessing.scale(np.array(x_data).reshape(-1)).reshape(-1, 10, standard_feature_matrix_len1, standard_feature_matrix_len2))\n",
    "    return x_data, y_data, standard_feature_matrix_len1, standard_feature_matrix_len2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機器學習模組\n",
    "\n",
    "# 資料集設定\n",
    "def data_setting(x_data, y_data, BATCH_SIZE):\n",
    "    print('\\n【資料集設定】\\n')\n",
    "    # 先转换成 torch 能识别的 Dataset\n",
    "    torch_dataset = Data.TensorDataset(torch.FloatTensor(x_data), torch.LongTensor(y_data))\n",
    "\n",
    "    # 把 dataset 放入 DataLoader\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    )\n",
    "    return loader\n",
    "# CNN_energy\n",
    "class CNN_energy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_energy, self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential( # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1, #  輸入信號的通道\n",
    "                out_channels = 4, # 卷積產生的通道\n",
    "                kernel_size = (3, 5), # 卷積核的尺寸\n",
    "                stride = 1, # 卷積步長\n",
    "                padding = (1,2) # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(4, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(4, 8, (3,5), 1, (1,2)), # output shape(8, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(8*10*16, 800)\n",
    "        self.hidden2 = nn.Linear(800, 800)\n",
    "        self.hidden3 = nn.Linear(800, 800)\n",
    "        self.out = nn.Linear(800, 13) # fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "# CNN_parameter\n",
    "class CNN_parameter(nn.Module):\n",
    "    def __init__(self, CNN_neurons_num, standard_feature_matrix_len1, standard_feature_matrix_len2):\n",
    "        super(CNN_parameter,self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential(  # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=10,  # 輸入信號的通道\n",
    "                out_channels=20,  # 卷積產生的通道\n",
    "                kernel_size=(3, 5),  # 卷積核的尺寸\n",
    "                stride=1,  # 卷積步長\n",
    "                padding=(1, 2)  # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(20, 16, 20)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(20, 40, (3, 5), 1, (1, 2)),  # output shape(40, 16, 20)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(40 * standard_feature_matrix_len1 * standard_feature_matrix_len2, CNN_neurons_num)\n",
    "        self.hidden2 = nn.Linear(CNN_neurons_num, CNN_neurons_num)\n",
    "        self.hidden3 = nn.Linear(CNN_neurons_num, CNN_neurons_num)\n",
    "        self.out = nn.Linear(CNN_neurons_num, 13)  # fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "# 訓練模型\n",
    "def module_training(loader, EPOCH, LR, module_class, standard_feature_matrix_len1,standard_feature_matrix_len2):\n",
    "    print('【訓練模型】')\n",
    "    module = module_class(800, standard_feature_matrix_len1, standard_feature_matrix_len2)\n",
    "    print(module)\n",
    "    # optimize all cnn parameters\n",
    "    optimizer = torch.optim.Adam(module.parameters(), lr=LR)\n",
    "    # the target label is not one-hotted\n",
    "    # pytorch 的 CrossEntropyLoss 會自動把張量轉為 one hot形式\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training and testing\n",
    "    for epoch in range(EPOCH):\n",
    "        print('第', epoch, '次訓練')\n",
    "        # enumerate : 枚舉可列舉對象．ex.\n",
    "        # A = [a, b, c]\n",
    "        # list(enumerate(A)) = [(0,a), (1,b), (2,c)]\n",
    "        for step, (b_x, b_y) in enumerate(loader):\n",
    "            output = module(b_x)\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step % 50 == 0):\n",
    "                print('step:' + str(step))\n",
    "                print('loss:' + str(loss))\n",
    "    print('訓練完成！！\\n')\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練結果分析\n",
    "def testing_report(module, testing_x_data, testing_y_data):\n",
    "    print('【訓練結果報告】')\n",
    "    test_output = module(torch.FloatTensor(testing_x_data))\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    # 混淆矩陣\n",
    "    confusion_matrix = metrics.confusion_matrix(testing_y_data,pred_y)\n",
    "    print('混淆矩陣')\n",
    "    print('true/predict')\n",
    "    print(confusion_matrix)\n",
    "    classification_report = metrics.classification_report(testing_y_data,pred_y)\n",
    "    print('\\n<============================>\\n\\n分類報告')\n",
    "    print(classification_report)\n",
    "\n",
    "    print('<============================>\\n\\n分類準確率')\n",
    "    correct = 0\n",
    "    for i in range(len(testing_y_data)):\n",
    "        if testing_y_data[i] == pred_y[i]:\n",
    "            correct += 1\n",
    "    print('測試資料數：', len(testing_y_data), ', 預測正確數：', correct, '準確率：', (correct/len(testing_y_data)))\n",
    "    return confusion_matrix, classification_report, (correct/len(testing_y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract_method = {'wavelet_with_energy_sum':wavelet_with_energy_sum, 'wavelet_parameters_only':wavelet_parameters_only}\n",
    "CNN_module = {'wavelet_with_energy_sum':CNN_energy, 'wavelet_parameters_only':CNN_parameter}\n",
    "\n",
    "def do_one_training(epoch, batch_size, CNN_neuron_num, sliding_window_size, sliding_window_movement, feature_extract, learning_rate, subject):\n",
    "    data = data_reading('S_All_A1_E1')\n",
    "    subject_emg, subject_restimulus = subject_choose(data, subject)\n",
    "    x_data, y_data = feature_extract_method[feature_extract](subject_emg, subject_restimulus, sliding_window_size, sliding_window_movement, 'db5', 'symmetric', 4)\n",
    "    # 將資料拆分成訓練與測試集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 0)\n",
    "    print('\\n資料拆分完成')\n",
    "    print('訓練資料: x -> ', len(x_train), ', y -> ', len(y_train))\n",
    "    print('測試資料: x -> ', len(x_test), ', y -> ', len(y_test))\n",
    "    loader = data_setting(x_train, y_train, batch_size)\n",
    "    module = module_training(loader, epoch, learning_rate, CNN_module[feature_extract])\n",
    "    print('\\n訓練資料結果分析\\n')\n",
    "    confusion_matrix_train, classification_report_train, accuracy_train = testing_report(module, x_train, y_train)\n",
    "    print('\\n測試資料結果分析\\n')\n",
    "    confusion_matrix_test, classification_report_test, accuracy_test = testing_report(module, x_test, y_test)\n",
    "    test_report_dict = {'epoch':epoch,\n",
    "                        'batch_size':batch_size, \n",
    "                        'module':module, 'sliding_window_size':sliding_window_size, \n",
    "                        'sliding_window_movement':sliding_window_movement, \n",
    "                        'feature_extract':feature_extract, \n",
    "                        'learning_rate':learning_rate, \n",
    "                        'subject':subject, \n",
    "                        'confusion_matrix_train':confusion_matrix_train, \n",
    "                        'classification_report_train':classification_report_train,\n",
    "                        'accuracy_train': accuracy_train, \n",
    "                        'confusion_matrix_test':confusion_matrix_test, \n",
    "                        'classification_report_test':classification_report_test,\n",
    "                        'accuracy_test': accuracy_test}\n",
    "#     path = 'training_report/DB1_training_report_' + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "    path = 'training_report/DB1_training_report_' + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + '.pickle'\n",
    "    file = open(path, 'wb')\n",
    "    pickle.dump(test_report_dict, file)\n",
    "    file.close()\n",
    "    print('\\n訓練完畢! 結果報告已匯入至 ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【讀取檔案】\n",
      "檔案讀取完畢！共讀取了 27 個檔案的資料\n",
      "\n",
      "【讀取受試者資料】\n",
      "受試者 1 資料讀取\n",
      "受試者 2 資料讀取\n",
      "受試者 3 資料讀取\n",
      "受試者 4 資料讀取\n",
      "受試者 5 資料讀取\n",
      "受試者 6 資料讀取\n",
      "受試者 7 資料讀取\n",
      "受試者 8 資料讀取\n",
      "受試者 9 資料讀取\n",
      "受試者 10 資料讀取\n",
      "受試者 11 資料讀取\n",
      "受試者 12 資料讀取\n",
      "受試者 13 資料讀取\n",
      "受試者 14 資料讀取\n",
      "受試者 15 資料讀取\n",
      "受試者 16 資料讀取\n",
      "受試者 17 資料讀取\n",
      "受試者 18 資料讀取\n",
      "受試者 19 資料讀取\n",
      "受試者 20 資料讀取\n",
      "受試者 21 資料讀取\n",
      "受試者 22 資料讀取\n",
      "受試者 23 資料讀取\n",
      "受試者 24 資料讀取\n",
      "受試者 25 資料讀取\n",
      "受試者 26 資料讀取\n",
      "受試者 27 資料讀取\n",
      "受試者資料讀取完畢！\n",
      "\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68247 筆資料\n",
      "\n",
      "資料標籤數量分布： [38611  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  32241 , y -> 32241\n",
      "資料標籤分佈： [2605 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487] \n",
      "\n",
      "正規化\n",
      "(32241, 10, 4, 26)\n",
      "\n",
      "資料拆分完成\n",
      "訓練資料: x ->  22568 , y ->  22568\n",
      "測試資料: x ->  9673 , y ->  9673\n",
      "\n",
      "【資料集設定】\n",
      "\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=4160, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5649, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4780, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4369, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4197, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4100, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3893, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4177, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4103, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3907, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3649, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3832, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4113, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3737, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3857, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3712, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3433, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3527, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3760, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3497, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3399, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3449, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3485, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3368, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3186, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3323, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3265, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3340, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3259, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2913, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3099, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3639, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4298, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3898, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3949, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3730, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4164, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3694, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3809, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4066, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4280, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4419, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4190, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3906, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3823, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3791, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3850, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3443, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3712, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3508, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3713, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3719, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4052, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3698, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4250, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4044, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4551, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4599, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4178, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4479, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4714, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4956, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4518, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4516, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4236, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4526, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4377, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4291, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5199, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5290, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5400, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5587, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5428, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4992, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5165, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5115, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5209, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5007, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5081, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4982, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5134, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4782, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5229, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4969, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4807, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5015, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4665, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4749, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4651, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4730, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4551, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4724, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4620, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4807, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4741, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4792, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4720, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4733, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4282, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4526, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4329, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "\n",
      "訓練資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[1231    0  136    0    0   25   60  132    0    0  149   64   22]\n",
      " [ 157    0  435    0    0  269  311  196    0    0   90  335   14]\n",
      " [ 210    0  970    0    0   80  163   21    0    0   85  209   10]\n",
      " [ 324    0  352    0    0  330  341  175    0    0  170  261   26]\n",
      " [  95    0  185    0    0  141  642  198    0    0   54  356   18]\n",
      " [ 128    0  129    0    0  875  118  145    0    0   58  186    3]\n",
      " [ 124    0  245    0    0   64 1051   89    0    0   27  144    6]\n",
      " [ 116    0  243    0    0  228  597  261    0    0   28  225    5]\n",
      " [ 382    0  249    0    0  156  444  126    0    0   75  383   11]\n",
      " [ 226    0  325    0    0  178  158   44    0    0  142  564   34]\n",
      " [ 465    0  215    0    0   41   62   15    0    0  476  302   37]\n",
      " [ 172    0  412    0    0   65   23    8    0    0   95  787   16]\n",
      " [ 446    0  274    0    0  282  134   24    0    0  157  354   72]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.68      0.42      1819\n",
      "           1       0.00      0.00      0.00      1807\n",
      "           2       0.23      0.55      0.33      1748\n",
      "           3       0.00      0.00      0.00      1979\n",
      "           4       0.00      0.00      0.00      1689\n",
      "           5       0.32      0.53      0.40      1642\n",
      "           6       0.26      0.60      0.36      1750\n",
      "           7       0.18      0.15      0.17      1703\n",
      "           8       0.00      0.00      0.00      1826\n",
      "           9       0.00      0.00      0.00      1671\n",
      "          10       0.30      0.30      0.30      1613\n",
      "          11       0.19      0.50      0.27      1578\n",
      "          12       0.26      0.04      0.07      1743\n",
      "\n",
      "    accuracy                           0.25     22568\n",
      "   macro avg       0.16      0.26      0.18     22568\n",
      "weighted avg       0.15      0.25      0.17     22568\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 22568 , 預測正確數： 5723 準確率： 0.25358915278270117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "測試資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[526   0  66   0   0   9  17  56   0   0  71  25  16]\n",
      " [ 73   0 187   0   0 115 142  59   0   0  41 143   2]\n",
      " [ 96   0 426   0   0  43  72   6   0   0  35  88   3]\n",
      " [142   0 140   0   0 154 155  66   0   0  80 102  12]\n",
      " [ 58   0  89   0   0  58 242  71   0   0  19 140   7]\n",
      " [ 55   0  49   0   0 401  50  70   0   0  22  82   3]\n",
      " [ 54   0  93   0   0  24 443  30   0   0  18  64   1]\n",
      " [ 66   0  97   0   0 109 264 115   0   0   7 131   2]\n",
      " [197   0  85   0   0  56 149  50   0   0  43 198  11]\n",
      " [102   0 171   0   0  69  61  14   0   0  45 236  17]\n",
      " [199   0  80   0   0   9  41  11   0   0 199 129  20]\n",
      " [ 74   0 185   0   0  23  11   2   0   0  35 298   7]\n",
      " [194   0 116   0   0 117  61   4   0   0  69 152  31]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.67      0.40       786\n",
      "           1       0.00      0.00      0.00       762\n",
      "           2       0.24      0.55      0.33       769\n",
      "           3       0.00      0.00      0.00       851\n",
      "           4       0.00      0.00      0.00       684\n",
      "           5       0.34      0.55      0.42       732\n",
      "           6       0.26      0.61      0.36       727\n",
      "           7       0.21      0.15      0.17       791\n",
      "           8       0.00      0.00      0.00       789\n",
      "           9       0.00      0.00      0.00       715\n",
      "          10       0.29      0.29      0.29       688\n",
      "          11       0.17      0.47      0.25       635\n",
      "          12       0.23      0.04      0.07       744\n",
      "\n",
      "    accuracy                           0.25      9673\n",
      "   macro avg       0.16      0.26      0.18      9673\n",
      "weighted avg       0.15      0.25      0.17      9673\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 9673 , 預測正確數： 2439 準確率： 0.25214514628346946\n",
      "\n",
      "訓練完畢! 結果報告已匯入至 training_reprot_wavelet_level/DB1_training_report_20190914_014459.pickle\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68249 筆資料\n",
      "\n",
      "資料標籤數量分布： [38613  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  32265 , y -> 32265\n",
      "資料標籤分佈： [2629 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487] \n",
      "\n",
      "正規化\n",
      "(32265, 10, 8, 17)\n",
      "\n",
      "資料拆分完成\n",
      "訓練資料: x ->  22585 , y ->  22585\n",
      "測試資料: x ->  9680 , y ->  9680\n",
      "\n",
      "【資料集設定】\n",
      "\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=5440, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5650, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4803, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4418, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4295, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4332, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4097, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3845, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3742, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3755, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3461, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3517, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3264, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3630, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3427, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3547, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3666, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3654, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3381, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3286, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3287, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3069, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3039, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3066, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2892, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2952, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2815, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2933, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2971, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2761, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2926, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2934, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2890, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2869, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2846, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2742, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2910, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3267, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2953, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3661, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3347, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3537, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3328, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3772, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3394, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3338, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3266, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2923, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3126, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3114, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3351, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3015, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2996, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2983, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2916, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2815, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2789, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3118, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2996, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2577, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3130, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3008, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2882, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2835, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2926, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2463, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2790, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2541, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2597, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2372, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2407, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2623, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2342, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2162, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3008, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3176, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2794, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2762, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2466, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2593, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2767, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2892, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2616, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2531, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2582, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2648, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2325, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2794, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3057, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2934, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2947, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2746, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2470, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2502, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3115, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3048, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3129, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3104, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3042, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "\n",
      "訓練資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[1006   37  159    3    6   12   68   44  182   21  259   36   10]\n",
      " [  95  769  360    7   39   64   66   64  145    2   62   41   91]\n",
      " [  82   44 1296    1    2   11   23   24  111   10   72   42   11]\n",
      " [ 240   61  402   34   28  301  110  110  407   20  100  123   55]\n",
      " [  74   88  193    1  471  190  185  192  105    1   57   82   22]\n",
      " [ 107   52   77    6    5  965   84  112  162   17   12   35   37]\n",
      " [  55   26  108    0   47   68  906   67  297   27   49   73   14]\n",
      " [ 125  123   72    6   42  194  164  648  276   24   17   38   31]\n",
      " [ 124   32   71   12    5   65   74   91 1159   22   63   26   32]\n",
      " [  88   59  210   10    2   90   44   83  271  209  256  228  120]\n",
      " [ 204   62  189    5    1   47   15   12  152   68  678   89   96]\n",
      " [  94   24  157   25    1   37   29   30  173   82  243  589  100]\n",
      " [ 220   94  292   22    0  121   24   58  164   58  208   74  405]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.55      0.46      1843\n",
      "           1       0.52      0.43      0.47      1805\n",
      "           2       0.36      0.75      0.49      1729\n",
      "           3       0.26      0.02      0.03      1991\n",
      "           4       0.73      0.28      0.41      1661\n",
      "           5       0.45      0.58      0.50      1671\n",
      "           6       0.51      0.52      0.51      1737\n",
      "           7       0.42      0.37      0.39      1760\n",
      "           8       0.32      0.65      0.43      1776\n",
      "           9       0.37      0.13      0.19      1670\n",
      "          10       0.33      0.42      0.37      1618\n",
      "          11       0.40      0.37      0.38      1584\n",
      "          12       0.40      0.23      0.29      1740\n",
      "\n",
      "    accuracy                           0.40     22585\n",
      "   macro avg       0.42      0.41      0.38     22585\n",
      "weighted avg       0.42      0.40      0.38     22585\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 22585 , 預測正確數： 9135 準確率： 0.404471994686739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "測試資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[434  23  75   0   2   8  16  11  75   7 126   8   1]\n",
      " [ 32 331 155   0  27  32  30  20  50   3  17  19  48]\n",
      " [ 36  18 578   0   2   7   8  17  52   9  36  24   1]\n",
      " [ 93  29 170  11   5 100  46  62 191  14  49  49  20]\n",
      " [ 39  31  81   0 198  81  79  87  46   2  21  41   6]\n",
      " [ 55  27  30   4   4 396  21  45  83   5   8   8  17]\n",
      " [ 24  11  40   1  21  35 356  41 134   9  17  46   5]\n",
      " [ 50  54  26   2  19  84  77 248 118   9   7  20  20]\n",
      " [ 58  15  37   4   3  28  40  36 538  16  30  13  21]\n",
      " [ 36  33  94   2   1  33  25  28 116  87 118 108  35]\n",
      " [112  30  84   4   0  15   5   5  54  26 258  42  48]\n",
      " [ 32  17  81  17   0   6  11  11  71  30  95 210  48]\n",
      " [ 93  45 108   9   0  59  21  25  72  30  89  32 164]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.55      0.46       786\n",
      "           1       0.50      0.43      0.46       764\n",
      "           2       0.37      0.73      0.49       788\n",
      "           3       0.20      0.01      0.02       839\n",
      "           4       0.70      0.28      0.40       712\n",
      "           5       0.45      0.56      0.50       703\n",
      "           6       0.48      0.48      0.48       740\n",
      "           7       0.39      0.34      0.36       734\n",
      "           8       0.34      0.64      0.44       839\n",
      "           9       0.35      0.12      0.18       716\n",
      "          10       0.30      0.38      0.33       683\n",
      "          11       0.34      0.33      0.34       629\n",
      "          12       0.38      0.22      0.28       747\n",
      "\n",
      "    accuracy                           0.39      9680\n",
      "   macro avg       0.40      0.39      0.37      9680\n",
      "weighted avg       0.40      0.39      0.36      9680\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 9680 , 預測正確數： 3809 準確率： 0.3934917355371901\n",
      "\n",
      "訓練完畢! 結果報告已匯入至 training_reprot_wavelet_level/DB1_training_report_20190914_030052.pickle\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68249 筆資料\n",
      "\n",
      "資料標籤數量分布： [38613  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  32207 , y -> 32207\n",
      "資料標籤分佈： [2571 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487] \n",
      "\n",
      "正規化\n",
      "(32207, 10, 16, 13)\n",
      "\n",
      "資料拆分完成\n",
      "訓練資料: x ->  22544 , y ->  22544\n",
      "測試資料: x ->  9663 , y ->  9663\n",
      "\n",
      "【資料集設定】\n",
      "\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=8320, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5649, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4881, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4492, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4246, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4114, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3998, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3973, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3912, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4128, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4111, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3891, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3804, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3724, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3961, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3795, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3832, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3874, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3959, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3593, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3682, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3976, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3671, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3504, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3651, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3518, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3703, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4163, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4165, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3846, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3568, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3474, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3524, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3383, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3215, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3465, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3329, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3114, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3104, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3402, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3409, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3386, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3232, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3318, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3026, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3377, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3441, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3231, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3145, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3066, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3368, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3215, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3252, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3141, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3081, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3391, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3792, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3479, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3796, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3570, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3547, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3343, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3323, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4180, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3990, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3917, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3980, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4014, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3639, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3615, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3334, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3443, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3231, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3916, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3521, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3397, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3449, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3568, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3318, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3490, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3455, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3569, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3207, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2963, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3048, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2914, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2891, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2911, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2990, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3012, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3482, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3396, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3197, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3694, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3441, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3314, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3052, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2976, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3175, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3064, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "\n",
      "訓練資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[1136   19  127   72   12   25   56    0  115    0  187   36    0]\n",
      " [  99  744  220  181  141  112   33    0  123    0  101   38    0]\n",
      " [  72   27 1341   50   11   34   21    0   75    0   77   51    0]\n",
      " [ 222   61  274  518   87  282  150    0  204    0  120   68    0]\n",
      " [  75   96  120   90  695  155  157    0  177    0   53   49    0]\n",
      " [ 129   87   94  112   69  888   65    0  166    0   17   20    0]\n",
      " [  95   20  120   52  151  102 1012    0  127    0   51   29    0]\n",
      " [ 153  106  149  130  105  260  379    1  387    0   31   63    0]\n",
      " [ 199   19  122  128   10   33  145    0 1065    0   37   48    0]\n",
      " [ 185   55  294   93   26  177  120    0  210    0  266  232    0]\n",
      " [ 345   32  173  145    4   77   46    0   89    0  567  132    0]\n",
      " [ 139    7  295   73   14   82  169    0  101    0  127  579    0]\n",
      " [ 368  108  319  151    6  264   67    0  123    0  200  119    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.64      0.45      1785\n",
      "           1       0.54      0.42      0.47      1792\n",
      "           2       0.37      0.76      0.50      1759\n",
      "           3       0.29      0.26      0.27      1986\n",
      "           4       0.52      0.42      0.46      1667\n",
      "           5       0.36      0.54      0.43      1647\n",
      "           6       0.42      0.58      0.48      1759\n",
      "           7       1.00      0.00      0.00      1764\n",
      "           8       0.36      0.59      0.45      1806\n",
      "           9       0.00      0.00      0.00      1658\n",
      "          10       0.31      0.35      0.33      1610\n",
      "          11       0.40      0.37      0.38      1586\n",
      "          12       0.00      0.00      0.00      1725\n",
      "\n",
      "    accuracy                           0.38     22544\n",
      "   macro avg       0.38      0.38      0.33     22544\n",
      "weighted avg       0.38      0.38      0.33     22544\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 22544 , 預測正確數： 8546 準確率： 0.3790809084457062\n",
      "\n",
      "測試資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[480  13  66  40   5  10  26   0  48   0  86  12   0]\n",
      " [ 34 302  90  98  53  50  17   0  62   0  62   9   0]\n",
      " [ 40  15 538  23   4  20   6   0  34   0  31  47   0]\n",
      " [ 81  19 132 192  39 109  66   0  98   0  63  45   0]\n",
      " [ 27  30  48  44 292  65  75   0  85   0  16  24   0]\n",
      " [ 68  37  55  50  38 373  25   0  71   0   3   7   0]\n",
      " [ 30   8  39  27  64  33 417   0  56   0  29  15   0]\n",
      " [ 60  39  61  52  53 123 156   0 151   0  11  24   0]\n",
      " [ 90  10  63  56   8  26  63   0 451   0  16  26   0]\n",
      " [ 82  20 115  53  19  72  51   0  86   0 119 111   0]\n",
      " [150  20  72  58   1  34  14   0  44   0 242  56   0]\n",
      " [ 45   0 134  34   5  37  62   0  46   0  39 225   0]\n",
      " [155  48 150  58   2 126  25   0  50   0  86  62   0]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.61      0.45       786\n",
      "           1       0.54      0.39      0.45       777\n",
      "           2       0.34      0.71      0.46       758\n",
      "           3       0.24      0.23      0.24       844\n",
      "           4       0.50      0.41      0.45       706\n",
      "           5       0.35      0.51      0.41       727\n",
      "           6       0.42      0.58      0.48       718\n",
      "           7       0.00      0.00      0.00       730\n",
      "           8       0.35      0.56      0.43       809\n",
      "           9       0.00      0.00      0.00       728\n",
      "          10       0.30      0.35      0.32       691\n",
      "          11       0.34      0.36      0.35       627\n",
      "          12       0.00      0.00      0.00       762\n",
      "\n",
      "    accuracy                           0.36      9663\n",
      "   macro avg       0.29      0.36      0.31      9663\n",
      "weighted avg       0.29      0.36      0.31      9663\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 9663 , 預測正確數： 3512 準確率： 0.3634482044913588\n",
      "\n",
      "訓練完畢! 結果報告已匯入至 training_reprot_wavelet_level/DB1_training_report_20190914_044002.pickle\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68249 筆資料\n",
      "\n",
      "資料標籤數量分布： [38613  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  32218 , y -> 32218\n",
      "資料標籤分佈： [2582 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487] \n",
      "\n",
      "正規化\n",
      "(32218, 10, 32, 11)\n",
      "\n",
      "資料拆分完成\n",
      "訓練資料: x ->  22552 , y ->  22552\n",
      "測試資料: x ->  9666 , y ->  9666\n",
      "\n",
      "【資料集設定】\n",
      "\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=14080, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5650, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4669, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4486, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4534, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4244, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4163, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4169, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4080, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4013, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3966, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4203, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4023, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3932, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3963, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4176, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3912, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3722, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4026, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3758, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3739, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4169, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4634, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4602, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4443, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4360, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4623, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4279, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4347, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4697, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4605, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4469, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4558, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4536, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4582, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4468, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4353, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4401, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4393, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4278, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4292, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4030, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4185, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4353, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4534, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4650, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4731, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4588, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4588, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4668, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4305, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4519, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4239, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4145, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4410, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4230, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4371, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4634, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4573, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4361, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4383, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4433, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4396, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4444, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4419, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4742, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4533, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4452, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4372, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4406, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4399, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4415, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4469, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3937, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3907, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3724, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3831, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3990, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3973, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4114, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4050, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4433, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4256, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4142, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3992, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4197, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3957, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4120, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4008, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4187, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4232, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4165, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4082, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4122, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3800, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3991, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3947, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4200, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3799, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3921, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3816, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "\n",
      "訓練資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[1217   30  170  110   15    0   58    0    0    0  167   39    0]\n",
      " [ 168  741  409  201  154    0   57    0    0    0   52   19    0]\n",
      " [ 179   51 1268  109    2    0   69    0    0    0   18   65    0]\n",
      " [ 342  168  319  792  104    0  123    0    0    0   83   60    0]\n",
      " [ 105  309  186  138  708    0   41    0    0    0   48   92    0]\n",
      " [ 154  305  192  719  137    0   70    0    0    0   66   44    0]\n",
      " [  97  120  166  174  183    0  784    0    0    0  102  122    0]\n",
      " [ 265  304  214  395  147    0  290    0    0    0   48   73    0]\n",
      " [ 300  113  127  639   62    0  345    0    0    0  114   95    0]\n",
      " [ 162  186  280  341   22    0   88    0    0    0  272  299    0]\n",
      " [ 363  141  163  197   20    0   20    0    0    0  590  134    0]\n",
      " [ 138   77  265  192    0    0   17    0    0    0  102  768    0]\n",
      " [ 303  290  335  333    8    0   43    0    0    0  286  165    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.67      0.43      1806\n",
      "           1       0.26      0.41      0.32      1801\n",
      "           2       0.31      0.72      0.43      1761\n",
      "           3       0.18      0.40      0.25      1991\n",
      "           4       0.45      0.44      0.44      1627\n",
      "           5       0.00      0.00      0.00      1687\n",
      "           6       0.39      0.45      0.42      1748\n",
      "           7       0.00      0.00      0.00      1736\n",
      "           8       0.00      0.00      0.00      1795\n",
      "           9       0.00      0.00      0.00      1650\n",
      "          10       0.30      0.36      0.33      1628\n",
      "          11       0.39      0.49      0.43      1559\n",
      "          12       0.00      0.00      0.00      1763\n",
      "\n",
      "    accuracy                           0.30     22552\n",
      "   macro avg       0.20      0.30      0.24     22552\n",
      "weighted avg       0.20      0.30      0.23     22552\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 22552 , 預測正確數： 6868 準確率： 0.3045406172401561\n",
      "\n",
      "測試資料結果分析\n",
      "\n",
      "【訓練結果報告】\n",
      "混淆矩陣\n",
      "true/predict\n",
      "[[529  13  74  39   2   0  28   0   0   0  69  22   0]\n",
      " [ 69 298 168  92  63   0  32   0   0   0  29  17   0]\n",
      " [ 68  19 568  36   3   0  26   0   0   0   6  30   0]\n",
      " [129  70 158 332  45   0  42   0   0   0  30  33   0]\n",
      " [ 42 140  74  59 323   0  27   0   0   0  20  61   0]\n",
      " [ 77 123  82 269  54   0  25   0   0   0  38  19   0]\n",
      " [ 32  52  68  95  68   0 324   0   0   0  28  62   0]\n",
      " [119 142  81 169  75   0 113   0   0   0  25  34   0]\n",
      " [148  43  63 299  27   0 125   0   0   0  58  57   0]\n",
      " [ 72  85 139 122   9   0  43   0   0   0 123 143   0]\n",
      " [147  61  78  78   7   0   9   0   0   0 237  56   0]\n",
      " [ 51  35 119  89   1   0   9   0   0   0  48 302   0]\n",
      " [126 109 141 146   4   0  16   0   0   0 108  74   0]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.68      0.44       776\n",
      "           1       0.25      0.39      0.30       768\n",
      "           2       0.31      0.75      0.44       756\n",
      "           3       0.18      0.40      0.25       839\n",
      "           4       0.47      0.43      0.45       746\n",
      "           5       0.00      0.00      0.00       687\n",
      "           6       0.40      0.44      0.42       729\n",
      "           7       0.00      0.00      0.00       758\n",
      "           8       0.00      0.00      0.00       820\n",
      "           9       0.00      0.00      0.00       736\n",
      "          10       0.29      0.35      0.32       673\n",
      "          11       0.33      0.46      0.39       654\n",
      "          12       0.00      0.00      0.00       724\n",
      "\n",
      "    accuracy                           0.30      9666\n",
      "   macro avg       0.20      0.30      0.23      9666\n",
      "weighted avg       0.20      0.30      0.23      9666\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 9666 , 預測正確數： 2913 準確率： 0.30136561142147733\n",
      "\n",
      "訓練完畢! 結果報告已匯入至 training_reprot_wavelet_level/DB1_training_report_20190914_081218.pickle\n",
      "【預處理-小波包係數】\n",
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68249 筆資料\n",
      "\n",
      "資料標籤數量分布： [38613  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n",
      "\n",
      "資料篩選完成\n",
      "資料數量： x ->  32162 , y -> 32162\n",
      "資料標籤分佈： [2526 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487] \n",
      "\n",
      "正規化\n",
      "(32162, 10, 64, 10)\n",
      "\n",
      "資料拆分完成\n",
      "訓練資料: x ->  22513 , y ->  22513\n",
      "測試資料: x ->  9649 , y ->  9649\n",
      "\n",
      "【資料集設定】\n",
      "\n",
      "【訓練模型】\n",
      "CNN_parameter(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(20, 40, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=25600, out_features=800, bias=True)\n",
      "  (hidden2): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (hidden3): Linear(in_features=800, out_features=800, bias=True)\n",
      "  (out): Linear(in_features=800, out_features=13, bias=True)\n",
      ")\n",
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericFang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5650, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4676, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4212, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3944, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3839, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3468, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3334, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3487, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3321, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3061, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2926, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3056, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3045, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2783, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2797, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2609, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2492, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2404, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2252, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2120, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2154, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2112, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1907, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1933, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2002, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1772, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1445, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1587, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1649, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1557, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1494, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1355, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1169, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1008, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1175, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0890, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1078, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0831, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0548, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0748, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0865, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0933, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0667, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0703, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0860, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0770, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0465, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0626, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0445, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0513, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0409, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0157, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0100, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0146, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0331, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0258, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0023, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9990, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0241, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0066, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0180, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0077, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0002, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9781, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9969, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9942, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9928, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9839, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9660, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9769, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9777, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9650, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9776, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9792, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9648, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9845, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9695, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9673, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9681, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9712, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9575, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9466, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9564, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9486, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9583, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9620, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9536, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9484, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9549, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9368, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9599, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9450, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9520, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9497, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9526, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9524, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9409, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9554, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9500, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(1.9560, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n",
      "\n",
      "\n",
      "訓練資料結果分析\n",
      "\n",
      "【訓練結果報告】\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 17289984000 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-804a71599d0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCNN_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wavelet_parameters_only'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandard_feature_matrix_len1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstandard_feature_matrix_len2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n訓練資料結果分析\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mconfusion_matrix_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesting_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n測試資料結果分析\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mconfusion_matrix_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesting_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a712faad20b3>\u001b[0m in \u001b[0;36mtesting_report\u001b[1;34m(module, testing_x_data, testing_y_data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtesting_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_x_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_y_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'【訓練結果報告】'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_x_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 混淆矩陣\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-19f73135838a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 展平多維卷積圖層 (batch_size, 32*10*16)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 17289984000 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "data = data_reading('S_All_A1_E1')\n",
    "subject = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "# subject = [1]\n",
    "feature_extract = 'wavelet_parameters_only'\n",
    "#sliding_window_size_list = [80, 100, 200, 300, 400, 500]\n",
    "max_level_list = [2,3,4,5,6]\n",
    "sliding_window_size = 80\n",
    "sliding_window_movement = 40\n",
    "batch_size = 2048\n",
    "learning_rate = 0.0005\n",
    "# epoch_list = [10, 20 ,30, 40, 50, 60, 70 ,80, 90, 100, 110, 120, 130, 140, 150]\n",
    "epoch = 100\n",
    "subject_emg, subject_restimulus = subject_choose(data, subject)\n",
    "for max_level in max_level_list:\n",
    "    sliding_window_movement = int(sliding_window_size / 2)\n",
    "    x_data, y_data,standard_feature_matrix_len1,standard_feature_matrix_len2 = feature_extract_method[feature_extract](subject_emg, subject_restimulus, sliding_window_size,\n",
    "                                                             sliding_window_movement, 'db5', 'symmetric', max_level)\n",
    "    # 將資料拆分成訓練與測試集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=0)\n",
    "    print('\\n資料拆分完成')\n",
    "    print('訓練資料: x -> ', len(x_train), ', y -> ', len(y_train))\n",
    "    print('測試資料: x -> ', len(x_test), ', y -> ', len(y_test))\n",
    "    loader = data_setting(x_train, y_train, batch_size)\n",
    "    module = module_training(loader, epoch, learning_rate, CNN_module['wavelet_parameters_only'], standard_feature_matrix_len1,standard_feature_matrix_len2)\n",
    "    print('\\n訓練資料結果分析\\n')\n",
    "    confusion_matrix_train, classification_report_train, accuracy_train = testing_report(module, x_train, y_train)\n",
    "    print('\\n測試資料結果分析\\n')\n",
    "    confusion_matrix_test, classification_report_test, accuracy_test = testing_report(module, x_test, y_test)\n",
    "    test_report_dict = {'epoch': epoch,\n",
    "                        'batch_size': batch_size,\n",
    "                        'module': module, 'sliding_window_size': sliding_window_size,\n",
    "                        'sliding_window_movement': sliding_window_movement,\n",
    "                        'feature_extract': feature_extract,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'subject': subject,\n",
    "                        'confusion_matrix_train': confusion_matrix_train,\n",
    "                        'classification_report_train': classification_report_train,\n",
    "                        'accuracy_train': accuracy_train,\n",
    "                        'confusion_matrix_test': confusion_matrix_test,\n",
    "                        'classification_report_test': classification_report_test,\n",
    "                        'accuracy_test': accuracy_test}\n",
    "    #     path = 'training_report/DB1_training_report_' + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "    path = 'training_reprot_wavelet_level/DB1_training_report_' + time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + '.pickle'\n",
    "    file = open(path, 'wb')\n",
    "    pickle.dump(test_report_dict, file)\n",
    "    file.close()\n",
    "    print('\\n訓練完畢! 結果報告已匯入至 ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
