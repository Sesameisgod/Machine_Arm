{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 library\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料讀取完畢！\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "path = 'S_All_A1_E1/'\n",
    "mat = 'S1_A1_E1'\n",
    "data = []\n",
    "for i in range(1,28,1):\n",
    "    file = path + 'S' + str(i) + '_A1_E1'\n",
    "    data.append(sio.loadmat(file))\n",
    "    plt.close()\n",
    "print('資料讀取完畢！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1受試者資料讀取\n",
      "受試者資料讀取完畢！\n"
     ]
    }
   ],
   "source": [
    "# 讀取受試者資料\n",
    "# subject_emg[受試者編號][emg編號][訊號]\n",
    "# subject_emg[受試者編號][狀態]\n",
    "subject_emg = []\n",
    "subject_restimulus = []\n",
    "# 選擇受試者\n",
    "subject_ID_begin = 0\n",
    "subject_ID_end = 1\n",
    "for sub_data in enumerate(data[subject_ID_begin:subject_ID_end]):\n",
    "    print('第' + str(sub_data[0]+1) + '受試者資料讀取')\n",
    "    subject_emg.append(sub_data[1]['emg'].T)\n",
    "    subject_restimulus.append(sub_data[1]['restimulus'])\n",
    "print('受試者資料讀取完畢！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定窗口大小與位移量(單位:sample point)\n",
    "sliding_window_size = 80\n",
    "sliding_duration = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1位受試者資料預處理\n",
      "資料預處理完畢！共 2525 筆資料\n",
      "資料標籤數量分布： [1588   94   62  104   76   96   78   63   77   64   80   62   81]\n"
     ]
    }
   ],
   "source": [
    "# 資料預處理\n",
    "import math\n",
    "import pywt\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "count = 0\n",
    "for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "    count += 1\n",
    "    print('第' + str(count) + '位受試者資料預處理')\n",
    "    total_len = len(restimulus)\n",
    "    # math.ceil -> 無條件進位\n",
    "    sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "\n",
    "    # 資料分割 + 特徵提取\n",
    "    window_begin = 0\n",
    "    for i in range(sliding_times):\n",
    "    #   特徵提取\n",
    "        feature_matrix = []\n",
    "        for e in emg:\n",
    "            emg_segment = e[window_begin:window_begin+sliding_window_size]\n",
    "        #   使用多階小波包轉換\n",
    "        #   小波包基底: db5\n",
    "        #   層數: 4\n",
    "            wp = pywt.WaveletPacket(data=emg_segment, wavelet='db5', mode='symmetric', maxlevel=4)\n",
    "        #   對第四層每一節點做能量值計算\n",
    "            wavelet_energy = []\n",
    "            for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                wavelet_energy.append(np.sum( (np.array(wp[j].data)) ** 2 ))\n",
    "            feature_matrix.append(wavelet_energy)\n",
    "        x_data.append(feature_matrix)\n",
    "    #   標標籤\n",
    "        restimulus_segment = restimulus[window_begin:window_begin+sliding_window_size]\n",
    "    #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "        counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "        #返回眾數(注意:此方法只有非負數列才可使用)\n",
    "        label_action_ID = np.argmax(counts)\n",
    "        y_data.append(label_action_ID)\n",
    "        window_begin = window_begin + sliding_duration\n",
    "print('資料預處理完畢！共', len(x_data), '筆資料')\n",
    "print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料篩選完成\n",
      "資料數量： x ->  1055 , y -> 1055\n",
      "資料標籤分佈： [118  94  62 104  76  96  78  63  77  64  80  62  81]\n"
     ]
    }
   ],
   "source": [
    "# 讓每一種標籤的資料都平均一點\n",
    "import random\n",
    "x_filter_data = []\n",
    "y_filter_data = []\n",
    "for i in range(len(x_data)):\n",
    "    if y_data[i] == 0:\n",
    "        if random.randint(1,15) == 1:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    else:\n",
    "        x_filter_data.append(x_data[i])\n",
    "        y_filter_data.append(y_data[i])\n",
    "x_data = x_filter_data\n",
    "y_data = y_filter_data\n",
    "del x_filter_data\n",
    "del y_filter_data\n",
    "print('資料篩選完成')\n",
    "print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "print('資料標籤分佈：', np.bincount(np.squeeze(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化\n",
    "from sklearn import preprocessing\n",
    "# 34095 x 10 x 16\n",
    "x_data = list(preprocessing.scale(np.array(x_data).reshape((-1, 16))).reshape(-1, 10, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_data)):\n",
    "    x_data[i] = [x_data[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# 訓練批次\n",
    "EPOCH = 200\n",
    "# 一個Batch的樣本數\n",
    "BATCH_SIZE = 1024\n",
    "# Learning rate\n",
    "LR = 0.0005\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = Data.TensorDataset(torch.FloatTensor(x_data), torch.LongTensor(y_data))\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(4, 8, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=1280, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (hidden3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential( # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1, # 輸入信號的通道\n",
    "                out_channels = 4, # 卷積產生的通道\n",
    "                kernel_size = (3, 5), # 卷積核的尺寸\n",
    "                stride = 1, # 卷積步長\n",
    "                padding = (1,2) # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(4, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(4, 8, (3,5), 1, (1,2)), # output shape(8, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(8*10*16, 256)\n",
    "        self.hidden2 = nn.Linear(256, 256)\n",
    "        self.hidden3 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, 13) # fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "    \n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次訓練\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.5648, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5580, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5448, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5281, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5228, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.5119, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4975, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4888, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4686, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4459, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4309, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4014, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3749, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3725, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3612, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3422, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3447, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3379, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3340, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3375, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3291, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3266, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3261, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3118, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3051, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2981, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2848, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2859, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2873, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2712, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2544, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2499, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2462, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2456, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2410, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2435, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2455, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2265, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2166, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2223, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2321, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2298, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2156, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2045, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2028, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2157, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2156, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2068, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1944, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1821, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1888, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1917, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1758, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1673, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1815, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2319, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2316, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1974, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1764, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1917, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1936, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1938, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1975, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2060, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2052, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2062, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2029, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2043, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1980, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1945, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1880, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1994, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1980, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1975, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1905, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1907, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1899, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1800, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1754, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1749, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1704, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1729, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1733, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1717, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1770, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1644, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1680, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1711, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1559, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1602, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1697, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1600, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1590, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1623, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1554, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1516, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1531, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1469, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1378, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1515, grad_fn=<NllLossBackward>)\n",
      "第 100 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1482, grad_fn=<NllLossBackward>)\n",
      "第 101 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1399, grad_fn=<NllLossBackward>)\n",
      "第 102 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1419, grad_fn=<NllLossBackward>)\n",
      "第 103 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1498, grad_fn=<NllLossBackward>)\n",
      "第 104 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1515, grad_fn=<NllLossBackward>)\n",
      "第 105 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1422, grad_fn=<NllLossBackward>)\n",
      "第 106 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1358, grad_fn=<NllLossBackward>)\n",
      "第 107 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1353, grad_fn=<NllLossBackward>)\n",
      "第 108 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1296, grad_fn=<NllLossBackward>)\n",
      "第 109 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1299, grad_fn=<NllLossBackward>)\n",
      "第 110 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1540, grad_fn=<NllLossBackward>)\n",
      "第 111 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1443, grad_fn=<NllLossBackward>)\n",
      "第 112 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1453, grad_fn=<NllLossBackward>)\n",
      "第 113 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1349, grad_fn=<NllLossBackward>)\n",
      "第 114 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1217, grad_fn=<NllLossBackward>)\n",
      "第 115 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1200, grad_fn=<NllLossBackward>)\n",
      "第 116 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1200, grad_fn=<NllLossBackward>)\n",
      "第 117 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1307, grad_fn=<NllLossBackward>)\n",
      "第 118 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1234, grad_fn=<NllLossBackward>)\n",
      "第 119 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1277, grad_fn=<NllLossBackward>)\n",
      "第 120 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1340, grad_fn=<NllLossBackward>)\n",
      "第 121 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1216, grad_fn=<NllLossBackward>)\n",
      "第 122 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1022, grad_fn=<NllLossBackward>)\n",
      "第 123 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1039, grad_fn=<NllLossBackward>)\n",
      "第 124 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1038, grad_fn=<NllLossBackward>)\n",
      "第 125 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1070, grad_fn=<NllLossBackward>)\n",
      "第 126 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1023, grad_fn=<NllLossBackward>)\n",
      "第 127 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1004, grad_fn=<NllLossBackward>)\n",
      "第 128 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1064, grad_fn=<NllLossBackward>)\n",
      "第 129 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1126, grad_fn=<NllLossBackward>)\n",
      "第 130 次訓練\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss:tensor(2.1132, grad_fn=<NllLossBackward>)\n",
      "第 131 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1114, grad_fn=<NllLossBackward>)\n",
      "第 132 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1036, grad_fn=<NllLossBackward>)\n",
      "第 133 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0996, grad_fn=<NllLossBackward>)\n",
      "第 134 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0960, grad_fn=<NllLossBackward>)\n",
      "第 135 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1051, grad_fn=<NllLossBackward>)\n",
      "第 136 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1040, grad_fn=<NllLossBackward>)\n",
      "第 137 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0924, grad_fn=<NllLossBackward>)\n",
      "第 138 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0888, grad_fn=<NllLossBackward>)\n",
      "第 139 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0845, grad_fn=<NllLossBackward>)\n",
      "第 140 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0867, grad_fn=<NllLossBackward>)\n",
      "第 141 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0871, grad_fn=<NllLossBackward>)\n",
      "第 142 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0837, grad_fn=<NllLossBackward>)\n",
      "第 143 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0850, grad_fn=<NllLossBackward>)\n",
      "第 144 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0917, grad_fn=<NllLossBackward>)\n",
      "第 145 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0859, grad_fn=<NllLossBackward>)\n",
      "第 146 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0830, grad_fn=<NllLossBackward>)\n",
      "第 147 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0855, grad_fn=<NllLossBackward>)\n",
      "第 148 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0864, grad_fn=<NllLossBackward>)\n",
      "第 149 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0829, grad_fn=<NllLossBackward>)\n",
      "第 150 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0850, grad_fn=<NllLossBackward>)\n",
      "第 151 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0811, grad_fn=<NllLossBackward>)\n",
      "第 152 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "第 153 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0743, grad_fn=<NllLossBackward>)\n",
      "第 154 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0703, grad_fn=<NllLossBackward>)\n",
      "第 155 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0833, grad_fn=<NllLossBackward>)\n",
      "第 156 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0751, grad_fn=<NllLossBackward>)\n",
      "第 157 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0758, grad_fn=<NllLossBackward>)\n",
      "第 158 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0811, grad_fn=<NllLossBackward>)\n",
      "第 159 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0859, grad_fn=<NllLossBackward>)\n",
      "第 160 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0895, grad_fn=<NllLossBackward>)\n",
      "第 161 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0833, grad_fn=<NllLossBackward>)\n",
      "第 162 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0844, grad_fn=<NllLossBackward>)\n",
      "第 163 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0853, grad_fn=<NllLossBackward>)\n",
      "第 164 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0837, grad_fn=<NllLossBackward>)\n",
      "第 165 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0825, grad_fn=<NllLossBackward>)\n",
      "第 166 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0776, grad_fn=<NllLossBackward>)\n",
      "第 167 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0690, grad_fn=<NllLossBackward>)\n",
      "第 168 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0701, grad_fn=<NllLossBackward>)\n",
      "第 169 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0735, grad_fn=<NllLossBackward>)\n",
      "第 170 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0724, grad_fn=<NllLossBackward>)\n",
      "第 171 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0798, grad_fn=<NllLossBackward>)\n",
      "第 172 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0869, grad_fn=<NllLossBackward>)\n",
      "第 173 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0936, grad_fn=<NllLossBackward>)\n",
      "第 174 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0914, grad_fn=<NllLossBackward>)\n",
      "第 175 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0808, grad_fn=<NllLossBackward>)\n",
      "第 176 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0700, grad_fn=<NllLossBackward>)\n",
      "第 177 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0799, grad_fn=<NllLossBackward>)\n",
      "第 178 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0908, grad_fn=<NllLossBackward>)\n",
      "第 179 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1002, grad_fn=<NllLossBackward>)\n",
      "第 180 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1041, grad_fn=<NllLossBackward>)\n",
      "第 181 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1150, grad_fn=<NllLossBackward>)\n",
      "第 182 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1353, grad_fn=<NllLossBackward>)\n",
      "第 183 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1508, grad_fn=<NllLossBackward>)\n",
      "第 184 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1600, grad_fn=<NllLossBackward>)\n",
      "第 185 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1577, grad_fn=<NllLossBackward>)\n",
      "第 186 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1595, grad_fn=<NllLossBackward>)\n",
      "第 187 次訓練\n",
      "step:0\n",
      "loss:tensor(2.1228, grad_fn=<NllLossBackward>)\n",
      "第 188 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0944, grad_fn=<NllLossBackward>)\n",
      "第 189 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0909, grad_fn=<NllLossBackward>)\n",
      "第 190 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0927, grad_fn=<NllLossBackward>)\n",
      "第 191 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0919, grad_fn=<NllLossBackward>)\n",
      "第 192 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0996, grad_fn=<NllLossBackward>)\n",
      "第 193 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0908, grad_fn=<NllLossBackward>)\n",
      "第 194 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0917, grad_fn=<NllLossBackward>)\n",
      "第 195 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0917, grad_fn=<NllLossBackward>)\n",
      "第 196 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "第 197 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0795, grad_fn=<NllLossBackward>)\n",
      "第 198 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0808, grad_fn=<NllLossBackward>)\n",
      "第 199 次訓練\n",
      "step:0\n",
      "loss:tensor(2.0835, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n"
     ]
    }
   ],
   "source": [
    "# optimize all cnn parameters\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "# the target label is not one-hotted\n",
    "# pytorch 的 CrossEntropyLoss 會自動把張量轉為 one hot形式\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    print('第', epoch, '次訓練')\n",
    "    # enumerate : 枚舉可列舉對象．ex.\n",
    "    # A = [a, b, c]\n",
    "    # list(enumerate(A)) = [(0,a), (1,b), (2,c)]\n",
    "    for step, (b_x, b_y) in enumerate(loader):\n",
    "        output = cnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step % 50 == 0):\n",
    "            print('step:' + str(step))\n",
    "            print('loss:' + str(loss))\n",
    "print('訓練完成！！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混淆矩陣\n",
      "true/predict\n",
      "[[90  2  1  7  0  6  0  2  0  0 10  0  0]\n",
      " [ 5 66  4 11  0  3  0  0  0  1  2  0  2]\n",
      " [ 2  3 53  1  0  0  0  0  0  0  0  0  3]\n",
      " [ 7 15  2 63  0 11  0  0  0  0  3  0  3]\n",
      " [ 0  8  0  1 61  5  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0 95  0  0  0  0  0  0  0]\n",
      " [ 1  6  3  8  5 13 42  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  2  0 56  0  0  1  0  0]\n",
      " [ 7  7  0  9  5 42  2  1  0  0  4  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0 59  0  0  2]\n",
      " [26  2  0  0  1 13  2  0  0  0 36  0  0]\n",
      " [ 2  0  0  0  0  0  2  0  0 49  0  0  9]\n",
      " [45  2  5  0  0  0  0  0  0  3  0  0 26]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.59       118\n",
      "           1       0.58      0.70      0.63        94\n",
      "           2       0.78      0.85      0.82        62\n",
      "           3       0.62      0.61      0.61       104\n",
      "           4       0.80      0.80      0.80        76\n",
      "           5       0.50      0.99      0.66        96\n",
      "           6       0.88      0.54      0.67        78\n",
      "           7       0.95      0.89      0.92        63\n",
      "           8       0.00      0.00      0.00        77\n",
      "           9       0.52      0.92      0.67        64\n",
      "          10       0.64      0.45      0.53        80\n",
      "          11       0.00      0.00      0.00        62\n",
      "          12       0.58      0.32      0.41        81\n",
      "\n",
      "    accuracy                           0.61      1055\n",
      "   macro avg       0.56      0.60      0.56      1055\n",
      "weighted avg       0.56      0.61      0.57      1055\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 1055 , 預測正確數： 647 準確率： 0.6132701421800948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\ericfang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "begin = 0\n",
    "end = len(y_data)\n",
    "# end = 500\n",
    "test_output = cnn(torch.FloatTensor(x_data[begin:end]))\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "y_cut_data = y_data[begin:end]\n",
    "# 混淆矩陣\n",
    "confusion_matrix = metrics.confusion_matrix(y_cut_data,pred_y)\n",
    "print('混淆矩陣')\n",
    "print('true/predict')\n",
    "print(confusion_matrix)\n",
    "classification_report = metrics.classification_report(y_cut_data,pred_y)\n",
    "print('\\n<============================>\\n\\n分類報告')\n",
    "print(classification_report)\n",
    "\n",
    "print('<============================>\\n\\n分類準確率')\n",
    "correct = 0\n",
    "for i in range(len(y_cut_data)):\n",
    "    if y_cut_data[i] == pred_y[i]:\n",
    "        correct += 1\n",
    "print('測試資料數：', len(y_cut_data), ', 預測正確數：', correct, '準確率：', (correct/len(y_cut_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "\n",
      "[-1.59325501 -1.30357228 -1.01388955 -0.72420682 -0.43452409 -0.14484136\n",
      "  0.14484136  0.43452409  0.72420682  1.01388955  1.30357228  1.59325501]\n",
      "\n",
      "\n",
      "[[-1.34164079 -1.34164079 -1.34164079]\n",
      " [-0.4472136  -0.4472136  -0.4472136 ]\n",
      " [ 0.4472136   0.4472136   0.4472136 ]\n",
      " [ 1.34164079  1.34164079  1.34164079]]\n",
      "range(0, 27)\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "z = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "zz = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "print(z)\n",
    "print('\\n')\n",
    "print(zz)\n",
    "print('\\n')\n",
    "print(preprocessing.scale(z))\n",
    "print('\\n')\n",
    "print(preprocessing.scale(zz))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
