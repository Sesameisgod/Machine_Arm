{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 library\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料讀取完畢！\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "path = 'S_All_A1_E1/'\n",
    "mat = 'S1_A1_E1'\n",
    "data = []\n",
    "for i in range(1,28,1):\n",
    "    file = path + 'S' + str(i) + '_A1_E1'\n",
    "    data.append(sio.loadmat(file))\n",
    "    plt.close()\n",
    "print('資料讀取完畢！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1受試者資料讀取\n",
      "第2受試者資料讀取\n",
      "第3受試者資料讀取\n",
      "第4受試者資料讀取\n",
      "第5受試者資料讀取\n",
      "第6受試者資料讀取\n",
      "第7受試者資料讀取\n",
      "第8受試者資料讀取\n",
      "第9受試者資料讀取\n",
      "第10受試者資料讀取\n",
      "第11受試者資料讀取\n",
      "第12受試者資料讀取\n",
      "第13受試者資料讀取\n",
      "第14受試者資料讀取\n",
      "第15受試者資料讀取\n",
      "第16受試者資料讀取\n",
      "第17受試者資料讀取\n",
      "第18受試者資料讀取\n",
      "第19受試者資料讀取\n",
      "第20受試者資料讀取\n",
      "第21受試者資料讀取\n",
      "第22受試者資料讀取\n",
      "第23受試者資料讀取\n",
      "第24受試者資料讀取\n",
      "第25受試者資料讀取\n",
      "第26受試者資料讀取\n",
      "第27受試者資料讀取\n",
      "受試者資料讀取完畢！\n"
     ]
    }
   ],
   "source": [
    "# 讀取受試者資料\n",
    "# subject_emg[受試者編號][emg編號][訊號]\n",
    "# subject_emg[受試者編號][狀態]\n",
    "subject_emg = []\n",
    "subject_restimulus = []\n",
    "for sub_data in enumerate(data):\n",
    "    print('第' + str(sub_data[0]+1) + '受試者資料讀取')\n",
    "    subject_emg.append(sub_data[1]['emg'].T)\n",
    "    subject_restimulus.append(sub_data[1]['restimulus'])\n",
    "print('受試者資料讀取完畢！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定窗口大小與位移量(單位:sample point)\n",
    "sliding_window_size = 80\n",
    "sliding_duration = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1位受試者資料預處理\n",
      "第2位受試者資料預處理\n",
      "第3位受試者資料預處理\n",
      "第4位受試者資料預處理\n",
      "第5位受試者資料預處理\n",
      "第6位受試者資料預處理\n",
      "第7位受試者資料預處理\n",
      "第8位受試者資料預處理\n",
      "第9位受試者資料預處理\n",
      "第10位受試者資料預處理\n",
      "第11位受試者資料預處理\n",
      "第12位受試者資料預處理\n",
      "第13位受試者資料預處理\n",
      "第14位受試者資料預處理\n",
      "第15位受試者資料預處理\n",
      "第16位受試者資料預處理\n",
      "第17位受試者資料預處理\n",
      "第18位受試者資料預處理\n",
      "第19位受試者資料預處理\n",
      "第20位受試者資料預處理\n",
      "第21位受試者資料預處理\n",
      "第22位受試者資料預處理\n",
      "第23位受試者資料預處理\n",
      "第24位受試者資料預處理\n",
      "第25位受試者資料預處理\n",
      "第26位受試者資料預處理\n",
      "第27位受試者資料預處理\n",
      "資料預處理完畢！共 68272 筆資料\n",
      "資料標籤數量分布： [38636  2569  2517  2830  2373  2374  2477  2494  2615  2386  2301  2213\n",
      "  2487]\n"
     ]
    }
   ],
   "source": [
    "# 資料預處理\n",
    "import math\n",
    "import pywt\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "count = 0\n",
    "for emg, restimulus in zip(subject_emg, subject_restimulus):\n",
    "    count += 1\n",
    "    print('第' + str(count) + '位受試者資料預處理')\n",
    "    total_len = len(restimulus)\n",
    "    # math.ceil -> 無條件進位\n",
    "    sliding_times = math.ceil((total_len - sliding_window_size) / sliding_duration) + 1\n",
    "\n",
    "    # 資料分割 + 特徵提取\n",
    "    window_begin = 0\n",
    "    for i in range(sliding_times):\n",
    "    #   特徵提取\n",
    "        feature_matrix = []\n",
    "        for e in emg:\n",
    "            emg_segment = e[window_begin:window_begin+sliding_window_size]\n",
    "        #   使用多階小波包轉換\n",
    "        #   小波包基底: db5\n",
    "        #   層數: 4\n",
    "            wp = pywt.WaveletPacket(data=emg_segment, wavelet='db5', mode='symmetric', maxlevel=4)\n",
    "        #   對第四層每一節點做能量值計算\n",
    "            wavelet_energy = []\n",
    "            for j in [node.path for node in wp.get_level(wp.maxlevel, 'natural')]:\n",
    "                wavelet_energy.append(np.sum( (np.array(wp[j].data)) ** 2 ))\n",
    "            feature_matrix.append(wavelet_energy)\n",
    "        x_data.append(feature_matrix)\n",
    "    #   標標籤\n",
    "        restimulus_segment = restimulus[window_begin:window_begin+sliding_window_size]\n",
    "    #   np.sqeeze()把矩陣內的單維向量的框框消掉\n",
    "        counts = np.bincount(np.squeeze(restimulus_segment))\n",
    "        #返回眾數(注意:此方法只有非負數列才可使用)\n",
    "        label_action_ID = np.argmax(counts)\n",
    "        y_data.append(label_action_ID)\n",
    "        window_begin = window_begin + sliding_duration\n",
    "print('資料預處理完畢！共', len(x_data), '筆資料')\n",
    "print('資料標籤數量分布：', np.bincount(np.squeeze(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料篩選完成\n",
      "資料數量： x ->  32128 , y -> 32128\n",
      "資料標籤分佈： [2492 2569 2517 2830 2373 2374 2477 2494 2615 2386 2301 2213 2487]\n"
     ]
    }
   ],
   "source": [
    "# 讓每一種標籤的資料都平均一點\n",
    "import random\n",
    "x_filter_data = []\n",
    "y_filter_data = []\n",
    "for i in range(len(x_data)):\n",
    "    if y_data[i] == 0:\n",
    "        if random.randint(1,15) == 1:\n",
    "            x_filter_data.append(x_data[i])\n",
    "            y_filter_data.append(y_data[i])\n",
    "    else:\n",
    "        x_filter_data.append(x_data[i])\n",
    "        y_filter_data.append(y_data[i])\n",
    "x_data = x_filter_data\n",
    "y_data = y_filter_data\n",
    "del x_filter_data\n",
    "del y_filter_data\n",
    "print('資料篩選完成')\n",
    "print('資料數量： x -> ', len(x_data), ', y ->', len(y_data))\n",
    "print('資料標籤分佈：', np.bincount(np.squeeze(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化\n",
    "from sklearn import preprocessing\n",
    "# 34095 x 10 x 16\n",
    "x_data = list(preprocessing.scale(np.array(x_data).reshape((-1, 16))).reshape(-1, 10, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_data)):\n",
    "    x_data[i] = [x_data[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# 訓練批次\n",
    "EPOCH = 100\n",
    "# 一個Batch的樣本數\n",
    "BATCH_SIZE = 512\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = Data.TensorDataset(torch.FloatTensor(x_data), torch.LongTensor(y_data))\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(4, 8, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hidden): Linear(in_features=1280, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (hidden3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # nn.Sequential()可以快速搭建神經網路\n",
    "        # 卷積運算所使用的mask一般稱為kernel map，這邊為5x5的map\n",
    "        # stride決定kernel map一次要走幾格\n",
    "        # 上面用5x5的kernel map去跑28x28的圖片，卷積完會只剩下26x26，故加兩層\n",
    "        # zero-padding 讓圖片大小相同\n",
    "        self.conv1 = nn.Sequential( # input shape(channel=1, height=28, weight=28)\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1, # 輸入信號的通道\n",
    "                out_channels = 4, # 卷積產生的通道\n",
    "                kernel_size = (3, 5), # 卷積核的尺寸\n",
    "                stride = 1, # 卷積步長\n",
    "                padding = (1,2) # 輸入的每一條邊補充0的層數\n",
    "            ),  # output shape(4, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input shape(4, 10, 16)\n",
    "            nn.Conv2d(4, 8, (3,5), 1, (1,2)), # output shape(8, 10, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Linear(8*10*16, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.hidden3 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 13) # fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # 展平多維卷積圖層 (batch_size, 32*10*16)\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = F.softmax(self.out(x))\n",
    "        return output\n",
    "    \n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3723, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:50\n",
      "loss:tensor(2.3927, grad_fn=<NllLossBackward>)\n",
      "第 1 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3668, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3828, grad_fn=<NllLossBackward>)\n",
      "第 2 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3633, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3698, grad_fn=<NllLossBackward>)\n",
      "第 3 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3445, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3967, grad_fn=<NllLossBackward>)\n",
      "第 4 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3684, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3398, grad_fn=<NllLossBackward>)\n",
      "第 5 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3408, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3714, grad_fn=<NllLossBackward>)\n",
      "第 6 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3667, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3833, grad_fn=<NllLossBackward>)\n",
      "第 7 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3812, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3783, grad_fn=<NllLossBackward>)\n",
      "第 8 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3468, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3666, grad_fn=<NllLossBackward>)\n",
      "第 9 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3239, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3372, grad_fn=<NllLossBackward>)\n",
      "第 10 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3610, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3593, grad_fn=<NllLossBackward>)\n",
      "第 11 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3641, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3763, grad_fn=<NllLossBackward>)\n",
      "第 12 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3449, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3740, grad_fn=<NllLossBackward>)\n",
      "第 13 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3634, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3732, grad_fn=<NllLossBackward>)\n",
      "第 14 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3621, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3168, grad_fn=<NllLossBackward>)\n",
      "第 15 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3716, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3382, grad_fn=<NllLossBackward>)\n",
      "第 16 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3595, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3595, grad_fn=<NllLossBackward>)\n",
      "第 17 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3629, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3571, grad_fn=<NllLossBackward>)\n",
      "第 18 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3307, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3677, grad_fn=<NllLossBackward>)\n",
      "第 19 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3465, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3785, grad_fn=<NllLossBackward>)\n",
      "第 20 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3691, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3551, grad_fn=<NllLossBackward>)\n",
      "第 21 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3331, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3617, grad_fn=<NllLossBackward>)\n",
      "第 22 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3481, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3338, grad_fn=<NllLossBackward>)\n",
      "第 23 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3590, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3706, grad_fn=<NllLossBackward>)\n",
      "第 24 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3823, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3361, grad_fn=<NllLossBackward>)\n",
      "第 25 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3570, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3337, grad_fn=<NllLossBackward>)\n",
      "第 26 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3426, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3721, grad_fn=<NllLossBackward>)\n",
      "第 27 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3546, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3548, grad_fn=<NllLossBackward>)\n",
      "第 28 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3362, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3437, grad_fn=<NllLossBackward>)\n",
      "第 29 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3592, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3279, grad_fn=<NllLossBackward>)\n",
      "第 30 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3685, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3823, grad_fn=<NllLossBackward>)\n",
      "第 31 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3572, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3197, grad_fn=<NllLossBackward>)\n",
      "第 32 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3440, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3442, grad_fn=<NllLossBackward>)\n",
      "第 33 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3371, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3452, grad_fn=<NllLossBackward>)\n",
      "第 34 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3312, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3217, grad_fn=<NllLossBackward>)\n",
      "第 35 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3530, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3369, grad_fn=<NllLossBackward>)\n",
      "第 36 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3494, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3539, grad_fn=<NllLossBackward>)\n",
      "第 37 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3317, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3326, grad_fn=<NllLossBackward>)\n",
      "第 38 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2933, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3742, grad_fn=<NllLossBackward>)\n",
      "第 39 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3222, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3445, grad_fn=<NllLossBackward>)\n",
      "第 40 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3075, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3267, grad_fn=<NllLossBackward>)\n",
      "第 41 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3574, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3449, grad_fn=<NllLossBackward>)\n",
      "第 42 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3335, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3583, grad_fn=<NllLossBackward>)\n",
      "第 43 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3508, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3500, grad_fn=<NllLossBackward>)\n",
      "第 44 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3533, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.2980, grad_fn=<NllLossBackward>)\n",
      "第 45 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3274, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3495, grad_fn=<NllLossBackward>)\n",
      "第 46 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3324, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3459, grad_fn=<NllLossBackward>)\n",
      "第 47 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3333, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3521, grad_fn=<NllLossBackward>)\n",
      "第 48 次訓練\n",
      "step:0\n",
      "loss:tensor(2.4129, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.2940, grad_fn=<NllLossBackward>)\n",
      "第 49 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3105, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3296, grad_fn=<NllLossBackward>)\n",
      "第 50 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3497, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3052, grad_fn=<NllLossBackward>)\n",
      "第 51 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3416, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3436, grad_fn=<NllLossBackward>)\n",
      "第 52 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2939, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3443, grad_fn=<NllLossBackward>)\n",
      "第 53 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3324, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3586, grad_fn=<NllLossBackward>)\n",
      "第 54 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3772, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3148, grad_fn=<NllLossBackward>)\n",
      "第 55 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3518, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3103, grad_fn=<NllLossBackward>)\n",
      "第 56 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3293, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3304, grad_fn=<NllLossBackward>)\n",
      "第 57 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3671, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3844, grad_fn=<NllLossBackward>)\n",
      "第 58 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3434, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3157, grad_fn=<NllLossBackward>)\n",
      "第 59 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3496, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3391, grad_fn=<NllLossBackward>)\n",
      "第 60 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3405, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3480, grad_fn=<NllLossBackward>)\n",
      "第 61 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3241, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3573, grad_fn=<NllLossBackward>)\n",
      "第 62 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3554, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3731, grad_fn=<NllLossBackward>)\n",
      "第 63 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3119, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3323, grad_fn=<NllLossBackward>)\n",
      "第 64 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3409, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3481, grad_fn=<NllLossBackward>)\n",
      "第 65 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3514, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3283, grad_fn=<NllLossBackward>)\n",
      "第 66 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3448, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3635, grad_fn=<NllLossBackward>)\n",
      "第 67 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3593, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3369, grad_fn=<NllLossBackward>)\n",
      "第 68 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3491, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3387, grad_fn=<NllLossBackward>)\n",
      "第 69 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3545, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3646, grad_fn=<NllLossBackward>)\n",
      "第 70 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3196, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:50\n",
      "loss:tensor(2.3770, grad_fn=<NllLossBackward>)\n",
      "第 71 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3301, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3638, grad_fn=<NllLossBackward>)\n",
      "第 72 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3401, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3271, grad_fn=<NllLossBackward>)\n",
      "第 73 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3521, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3620, grad_fn=<NllLossBackward>)\n",
      "第 74 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3353, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3017, grad_fn=<NllLossBackward>)\n",
      "第 75 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3094, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3112, grad_fn=<NllLossBackward>)\n",
      "第 76 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3392, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3298, grad_fn=<NllLossBackward>)\n",
      "第 77 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3480, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3502, grad_fn=<NllLossBackward>)\n",
      "第 78 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3164, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3534, grad_fn=<NllLossBackward>)\n",
      "第 79 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3568, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3036, grad_fn=<NllLossBackward>)\n",
      "第 80 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3388, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3263, grad_fn=<NllLossBackward>)\n",
      "第 81 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3105, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3191, grad_fn=<NllLossBackward>)\n",
      "第 82 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3224, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3267, grad_fn=<NllLossBackward>)\n",
      "第 83 次訓練\n",
      "step:0\n",
      "loss:tensor(2.2975, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3574, grad_fn=<NllLossBackward>)\n",
      "第 84 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3501, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3209, grad_fn=<NllLossBackward>)\n",
      "第 85 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3468, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.2929, grad_fn=<NllLossBackward>)\n",
      "第 86 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3118, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3305, grad_fn=<NllLossBackward>)\n",
      "第 87 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3059, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3444, grad_fn=<NllLossBackward>)\n",
      "第 88 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3949, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3543, grad_fn=<NllLossBackward>)\n",
      "第 89 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3005, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3408, grad_fn=<NllLossBackward>)\n",
      "第 90 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3155, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3482, grad_fn=<NllLossBackward>)\n",
      "第 91 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3556, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3517, grad_fn=<NllLossBackward>)\n",
      "第 92 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3391, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3210, grad_fn=<NllLossBackward>)\n",
      "第 93 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3011, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3204, grad_fn=<NllLossBackward>)\n",
      "第 94 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3152, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3322, grad_fn=<NllLossBackward>)\n",
      "第 95 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3084, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3218, grad_fn=<NllLossBackward>)\n",
      "第 96 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3053, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3136, grad_fn=<NllLossBackward>)\n",
      "第 97 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3220, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3269, grad_fn=<NllLossBackward>)\n",
      "第 98 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3297, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3553, grad_fn=<NllLossBackward>)\n",
      "第 99 次訓練\n",
      "step:0\n",
      "loss:tensor(2.3668, grad_fn=<NllLossBackward>)\n",
      "step:50\n",
      "loss:tensor(2.3101, grad_fn=<NllLossBackward>)\n",
      "訓練完成！！\n"
     ]
    }
   ],
   "source": [
    "# optimize all cnn parameters\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "# the target label is not one-hotted\n",
    "# pytorch 的 CrossEntropyLoss 會自動把張量轉為 one hot形式\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    print('第', epoch, '次訓練')\n",
    "    # enumerate : 枚舉可列舉對象．ex.\n",
    "    # A = [a, b, c]\n",
    "    # list(enumerate(A)) = [(0,a), (1,b), (2,c)]\n",
    "    for step, (b_x, b_y) in enumerate(loader):\n",
    "        output = cnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step % 50 == 0):\n",
    "            print('step:' + str(step))\n",
    "            print('loss:' + str(loss))\n",
    "print('訓練完成！！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混淆矩陣\n",
      "true/predict\n",
      "[[1308   38  195  151   74   65   40   52   34    3  396   96   40]\n",
      " [ 116  906  276  242  325  204   55  183    5   13   91   62   91]\n",
      " [  83  182 1350  277   80   48   58   66    3   16  151  185   18]\n",
      " [ 181  199  281  867  264  215  124  158   21   40  150  222  108]\n",
      " [  61  184   62  173 1255  203   59  127    1   20   66   76   86]\n",
      " [ 107  128   44  220  108 1222   48  137   21   46   37   75  181]\n",
      " [ 164   69  136  135  358  107  880  192  118   28   45  121  124]\n",
      " [ 175  151  119  212  271  268  134  766   30   46   26   87  209]\n",
      " [ 296   91  117  355  162  219  163  282  441   33  142  167  147]\n",
      " [ 150  154  144  192  111  126   64  114   68  312  352  362  237]\n",
      " [ 378  137  118  228  105   41   65   41   34   36  716  245  157]\n",
      " [ 120   81  144   90   46   64   23   78   20  120  218 1046  163]\n",
      " [ 335  174  256  207   55  147    6   87    3   32  318  235  632]]\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類報告\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.52      0.44      2492\n",
      "           1       0.36      0.35      0.36      2569\n",
      "           2       0.42      0.54      0.47      2517\n",
      "           3       0.26      0.31      0.28      2830\n",
      "           4       0.39      0.53      0.45      2373\n",
      "           5       0.42      0.51      0.46      2374\n",
      "           6       0.51      0.36      0.42      2477\n",
      "           7       0.34      0.31      0.32      2494\n",
      "           8       0.55      0.17      0.26      2615\n",
      "           9       0.42      0.13      0.20      2386\n",
      "          10       0.26      0.31      0.29      2301\n",
      "          11       0.35      0.47      0.40      2213\n",
      "          12       0.29      0.25      0.27      2487\n",
      "\n",
      "    accuracy                           0.36     32128\n",
      "   macro avg       0.38      0.37      0.35     32128\n",
      "weighted avg       0.38      0.36      0.35     32128\n",
      "\n",
      "<============================>\n",
      "\n",
      "分類準確率\n",
      "測試資料數： 32128 , 預測正確數： 11701 準確率： 0.36419945219123506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "begin = 0\n",
    "end = len(y_data)\n",
    "# end = 500\n",
    "test_output = cnn(torch.FloatTensor(x_data[begin:end]))\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "y_cut_data = y_data[begin:end]\n",
    "# 混淆矩陣\n",
    "confusion_matrix = metrics.confusion_matrix(y_cut_data,pred_y)\n",
    "print('混淆矩陣')\n",
    "print('true/predict')\n",
    "print(confusion_matrix)\n",
    "classification_report = metrics.classification_report(y_cut_data,pred_y)\n",
    "print('\\n<============================>\\n\\n分類報告')\n",
    "print(classification_report)\n",
    "\n",
    "print('<============================>\\n\\n分類準確率')\n",
    "correct = 0\n",
    "for i in range(len(y_cut_data)):\n",
    "    if y_cut_data[i] == pred_y[i]:\n",
    "        correct += 1\n",
    "print('測試資料數：', len(y_cut_data), ', 預測正確數：', correct, '準確率：', (correct/len(y_cut_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
